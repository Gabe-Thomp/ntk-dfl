# device: "cuda" | "cpu"
device: cuda
debug: true

gpu_intensive: false
jac_calc_intensive: true
deq_intensive: true

jac_batch_size: 20

# distributed networks
# users:            number of users
rounds: 20
users: 250
part_rate: .08

batch_m: 5


lr: 0.001
lr_end: 0.00001

local_batch_size: 200
loss: "ce"

# stuff for self training
self_train: null
verbose: false
sgd_batch_size: 4
sgd_lr: 0.01
sgd_epochs: 10

taus:
- 100
- 200
- 300 
- 400
- 500
# - 600
# - 700
# - 800
# - 900
# - 1000
# - 1500
# - 2000
# - 2500
# - 3000
# - 3500

# Dataset configurations
# train_data_dir: data/mnist/train.dat
# test_data_dir:  data/mnist/test.dat

train_data_dir: data/cifar/train.dat
test_data_dir:  data/cifar/test.dat

# train_data_dir: data/fmnist/train.dat
# test_data_dir:  data/fmnist/test.dat

# train_data_dir: data/emnist/digits/train.dat
# test_data_dir:  data/emnist/digits/test.dat
# user_with_data: ""

# user_with_data: "data/user_with_data/emnist/digits/user_dataidx_map_0.dat"

user_with_data: ""
iid: true

# alpha: 0.1
# user_with_data: "data/user_with_data/fmnist300/a0.1/user_dataidx_map_0.10_0.dat"
# iid: false



datapoint_size:
- 32
- 32
channels: 3
label_size: 10

# Log and record configurations
record_dir:  "../records/trials/trial26/{}.dat"
log_level:   "INFO"
log_file:    "records/trials/trial26/train.log"

model: "cnn"
full_weight_dir: ""