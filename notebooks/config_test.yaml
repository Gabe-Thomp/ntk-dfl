# For a trial on IID fashion-mnist with the convolutional NN

# device: "cuda" | "cpu"
device: cuda
debug: true

# Toggles all on or off
gpu_intensive: false

# Individual toggles
jac_calc_intensive: true
deq_intensive: true
ntk_intensive: false

# distributed networks
# users:            number of users
rounds: 5
users: 300

# Selects 20 per round
part_rate: 1.0 #0.07

# Select p for Erdos-Renyi graph
# users*p is the mean number of edges per node
# Mean 9 users per node
# p: 0.015


sgd_epochs: 5
sgd_lr: 0.0001
sgd_batch_size: 64

self_train: false

topology: "regular"
d: 5

train_on_own_data: false
own_data_all_rounds: false

graph_name: null
verbose: true

record_path: null

lr: 0.001
lr_end: 0.0001
# Hard coded based on training data set size. For example, 60000 fashion mnist images/300 users = 200 batch size
# I think this only applies to 

local_batch_size: 166


# Things that should make larger models work
jac_batch_size: 83


# Whether or not weights should be initialized to the same value for each client originally
same_init: true

loss: "ce"

taus:
- 100
- 200
- 300 
- 400
- 500
- 600
- 700
- 800
# - 900
- 1000
# - 1500
# - 2000

# Dataset configurations
train_data_dir: data/cifar/train.dat
test_data_dir:  data/cifar/test.dat

# I think user_with_data is the file that contains the user data parsed into non IID segments already
# This file is alpha0.1 

# user_with_data: "data/user_with_data/fmnist300/a0.5/user_dataidx_map_0.50_0.dat"
# iid: false


# train_data_dir: data/mnist/train.dat
# test_data_dir:  data/mnist/test.dat
user_with_data: ""
iid: true

# user_with_data: "data/user_with_data/mnist300/a0.1/user_dataidx_map_0.10_0.dat"
# iid: false

datapoint_size:
- 32
- 32
channels: 3
label_size: 10

# Log and record configurations
record_dir:  "../records/trials/trial25/{}.dat"
log_level:   "INFO"
log_file:    "records/trials/trial25/train.log"

model: "cnn"
full_weight_dir: ""

