{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "directory_paths = [\"../../../\", \"/home/gathomp3/Deep_Learning/NeuralTangent/ntk-fed/notebooks/baselines/dfedsam/DP-FedSAM\"]\n",
    "\n",
    "for directory_path in directory_paths:\n",
    "    if directory_path not in sys.path:\n",
    "        # Add the directory to sys.path\n",
    "        sys.path.append(directory_path)\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils import data\n",
    "from torch import optim\n",
    "\n",
    "from utils.utils import *\n",
    "from utils import load_config\n",
    "from utils.validate import *\n",
    "\n",
    "from fedlearning.topology import *\n",
    "from fedlearning.model import *\n",
    "from fedlearning.dataset import *\n",
    "from fedlearning.evolve import *\n",
    "from fedlearning.optimizer import GlobalUpdater, LocalUpdater, get_omegas\n",
    "from fedlearning.quantizer import SqcCompressor\n",
    "\n",
    "from fedml_api.dpfedsam.sam import SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy array): Array of data samples.\n",
    "            targets (numpy array): Array of labels corresponding to the data samples.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, target\n",
    "\n",
    "def numpy_to_tensor_transform(data):\n",
    "    return torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Loaded configuration from baseline_configs/config_dfedavgm.yaml\n",
      "Dataset path: ../../../data/mnist/train.dat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating config from filepath:  baseline_configs/config_dfedavgm.yaml\n",
      "/home/gathomp3/Deep_Learning/NeuralTangent/ntk-fed/notebooks/baselines/dfedsam/../../../../records/baseline_trials/dfedavgm/trial_test/train.log\n"
     ]
    }
   ],
   "source": [
    "config_file = \"baseline_configs/config_dfedavgm.yaml\"\n",
    "config = load_config(config_file)\n",
    "\n",
    "logger = init_logger(config)\n",
    "logger.info(\"Loaded configuration from {}\".format(config_file))\n",
    "logger.info(\"Dataset path: {}\".format(config.train_data_dir))\n",
    "\n",
    "# Define a model to extract number of parameters for record\n",
    "if config.record_path is not None:\n",
    "    record = load_record(config.record_path)\n",
    "    logger.info(\"Loaded record from {}\".format(config.record_path))\n",
    "    loaded_record = True\n",
    "else:\n",
    "    model = init_model(config, logger)\n",
    "    record = init_record(config, model)\n",
    "    loaded_record = False\n",
    "\n",
    "if config.device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Non-IID data distribution\n",
      "Load user_with_data from /home/gathomp3/Deep_Learning/NeuralTangent/ntk-fed/data/user_with_data/mnist300/a0.1/user_dataidx_map_0.10_0.dat\n"
     ]
    }
   ],
   "source": [
    "# Create user_ids\n",
    "user_ids = np.arange(0, config.users)\n",
    "# load the dataset\n",
    "# dataset object is a dictionary with keys: train_data, test_data, user_with_data\n",
    "# user_with_data is a dictionary with keys: userID:sampleID\n",
    "# For example, in the IID setting ID's are just assigned like 0, 1, 2, 3, ...\n",
    "dataset = assign_user_data(config, logger)\n",
    "test_images = torch.from_numpy(dataset[\"test_data\"][\"images\"]).to(config.device)\n",
    "test_labels = torch.from_numpy(dataset[\"test_data\"][\"labels\"]).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_record = False\n",
    "# Create a dictionary of models for each user\n",
    "# Same initialization for all users\n",
    "# If record/model_dict is passed, continue training from where it left off\n",
    "if loaded_record == True:\n",
    "    model_dict = record[\"models\"]\n",
    "else:\n",
    "    if config.same_init:\n",
    "        model = init_model(config, logger)\n",
    "        model_dict = {model_id: copy.deepcopy(model) for model_id in user_ids}\n",
    "    else:\n",
    "        model_dict = {model_id: init_model(config, logger) for model_id in user_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get zeroth round loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging initial loss, acc\n",
      "client 0 loss 3.0066 acc 0.1292\n",
      "client 1 loss 4.4558 acc 0.1292\n",
      "client 2 loss 2.8617 acc 0.1292\n",
      "client 3 loss 3.3323 acc 0.1292\n",
      "client 4 loss 3.1203 acc 0.1292\n",
      "client 5 loss 2.4805 acc 0.1292\n",
      "client 6 loss 3.9518 acc 0.1292\n",
      "client 7 loss 2.5914 acc 0.1292\n",
      "client 8 loss 2.7790 acc 0.1292\n",
      "client 9 loss 2.7972 acc 0.1292\n",
      "client 10 loss 4.6281 acc 0.1292\n",
      "client 11 loss 3.2511 acc 0.1292\n",
      "client 12 loss 3.0626 acc 0.1292\n",
      "client 13 loss 3.6284 acc 0.1292\n",
      "client 14 loss 1.3556 acc 0.1292\n",
      "client 15 loss 2.5742 acc 0.1292\n",
      "client 16 loss 4.3688 acc 0.1292\n",
      "client 17 loss 2.3614 acc 0.1292\n",
      "client 18 loss 2.4626 acc 0.1292\n",
      "client 19 loss 2.0634 acc 0.1292\n",
      "client 20 loss 2.5206 acc 0.1292\n",
      "client 21 loss 2.9010 acc 0.1292\n",
      "client 22 loss 2.4896 acc 0.1292\n",
      "client 23 loss 1.8029 acc 0.1292\n",
      "client 24 loss 2.2095 acc 0.1292\n",
      "client 25 loss 3.1305 acc 0.1292\n",
      "client 26 loss 3.7918 acc 0.1292\n",
      "client 27 loss 2.6444 acc 0.1292\n",
      "client 28 loss 3.0115 acc 0.1292\n",
      "client 29 loss 2.2583 acc 0.1292\n",
      "client 30 loss 2.0971 acc 0.1292\n",
      "client 31 loss 3.2779 acc 0.1292\n",
      "client 32 loss 3.3570 acc 0.1292\n",
      "client 33 loss 2.7805 acc 0.1292\n",
      "client 34 loss 2.4119 acc 0.1292\n",
      "client 35 loss 3.0871 acc 0.1292\n",
      "client 36 loss 4.2302 acc 0.1292\n",
      "client 37 loss 4.4024 acc 0.1292\n",
      "client 38 loss 1.9819 acc 0.1292\n",
      "client 39 loss 2.7751 acc 0.1292\n",
      "client 40 loss 4.0983 acc 0.1292\n",
      "client 41 loss 3.7498 acc 0.1292\n",
      "client 42 loss 1.9427 acc 0.1292\n",
      "client 43 loss 4.0281 acc 0.1292\n",
      "client 44 loss 1.8875 acc 0.1292\n",
      "client 45 loss 3.5362 acc 0.1292\n",
      "client 46 loss 3.1158 acc 0.1292\n",
      "client 47 loss 4.5523 acc 0.1292\n",
      "client 48 loss 1.3059 acc 0.1292\n",
      "client 49 loss 1.7123 acc 0.1292\n",
      "client 50 loss 3.2863 acc 0.1292\n",
      "client 51 loss 2.4171 acc 0.1292\n",
      "client 52 loss 1.5002 acc 0.1292\n",
      "client 53 loss 3.3362 acc 0.1292\n",
      "client 54 loss 4.4989 acc 0.1292\n",
      "client 55 loss 2.9250 acc 0.1292\n",
      "client 56 loss 2.6379 acc 0.1292\n",
      "client 57 loss 2.5678 acc 0.1292\n",
      "client 58 loss 2.2816 acc 0.1292\n",
      "client 59 loss 2.7241 acc 0.1292\n",
      "client 60 loss 1.7600 acc 0.1292\n",
      "client 61 loss 3.3394 acc 0.1292\n",
      "client 62 loss 2.2158 acc 0.1292\n",
      "client 63 loss 1.3564 acc 0.1292\n",
      "client 64 loss 3.1283 acc 0.1292\n",
      "client 65 loss 1.9029 acc 0.1292\n",
      "client 66 loss 1.6091 acc 0.1292\n",
      "client 67 loss 2.4259 acc 0.1292\n",
      "client 68 loss 2.8406 acc 0.1292\n",
      "client 69 loss 4.4001 acc 0.1292\n",
      "client 70 loss 3.6528 acc 0.1292\n",
      "client 71 loss 3.6628 acc 0.1292\n",
      "client 72 loss 2.2960 acc 0.1292\n",
      "client 73 loss 2.3685 acc 0.1292\n",
      "client 74 loss 2.0944 acc 0.1292\n",
      "client 75 loss 3.7846 acc 0.1292\n",
      "client 76 loss 3.7720 acc 0.1292\n",
      "client 77 loss 1.7104 acc 0.1292\n",
      "client 78 loss 3.4213 acc 0.1292\n",
      "client 79 loss 2.3605 acc 0.1292\n",
      "client 80 loss 2.8859 acc 0.1292\n",
      "client 81 loss 4.3462 acc 0.1292\n",
      "client 82 loss 3.0217 acc 0.1292\n",
      "client 83 loss 2.5736 acc 0.1292\n",
      "client 84 loss 2.7022 acc 0.1292\n",
      "client 85 loss 4.5243 acc 0.1292\n",
      "client 86 loss 2.5139 acc 0.1292\n",
      "client 87 loss 3.1305 acc 0.1292\n",
      "client 88 loss 3.2382 acc 0.1292\n",
      "client 89 loss 1.3659 acc 0.1292\n",
      "client 90 loss 3.1365 acc 0.1292\n",
      "client 91 loss 2.7616 acc 0.1292\n",
      "client 92 loss 2.8910 acc 0.1292\n",
      "client 93 loss 3.3678 acc 0.1292\n",
      "client 94 loss 2.7717 acc 0.1292\n",
      "client 95 loss 3.2791 acc 0.1292\n",
      "client 96 loss 2.1988 acc 0.1292\n",
      "client 97 loss 2.6704 acc 0.1292\n",
      "client 98 loss 1.7073 acc 0.1292\n",
      "client 99 loss 2.7090 acc 0.1292\n",
      "client 100 loss 2.4887 acc 0.1292\n",
      "client 101 loss 3.1814 acc 0.1292\n",
      "client 102 loss 2.9708 acc 0.1292\n",
      "client 103 loss 2.2850 acc 0.1292\n",
      "client 104 loss 3.7611 acc 0.1292\n",
      "client 105 loss 3.0315 acc 0.1292\n",
      "client 106 loss 3.0142 acc 0.1292\n",
      "client 107 loss 2.0901 acc 0.1292\n",
      "client 108 loss 3.8970 acc 0.1292\n",
      "client 109 loss 1.9576 acc 0.1292\n",
      "client 110 loss 1.7344 acc 0.1292\n",
      "client 111 loss 1.8594 acc 0.1292\n",
      "client 112 loss 1.9880 acc 0.1292\n",
      "client 113 loss 4.6316 acc 0.1292\n",
      "client 114 loss 2.7593 acc 0.1292\n",
      "client 115 loss 2.6776 acc 0.1292\n",
      "client 116 loss 3.8428 acc 0.1292\n",
      "client 117 loss 2.3222 acc 0.1292\n",
      "client 118 loss 3.5247 acc 0.1292\n",
      "client 119 loss 2.7635 acc 0.1292\n",
      "client 120 loss 3.4412 acc 0.1292\n",
      "client 121 loss 2.9695 acc 0.1292\n",
      "client 122 loss 2.3745 acc 0.1292\n",
      "client 123 loss 1.9525 acc 0.1292\n",
      "client 124 loss 2.3565 acc 0.1292\n",
      "client 125 loss 2.2491 acc 0.1292\n",
      "client 126 loss 1.7013 acc 0.1292\n",
      "client 127 loss 2.9731 acc 0.1292\n",
      "client 128 loss 1.6991 acc 0.1292\n",
      "client 129 loss 3.2628 acc 0.1292\n",
      "client 130 loss 2.8352 acc 0.1292\n",
      "client 131 loss 2.7468 acc 0.1292\n",
      "client 132 loss 3.9375 acc 0.1292\n",
      "client 133 loss 2.6216 acc 0.1292\n",
      "client 134 loss 2.9419 acc 0.1292\n",
      "client 135 loss 2.8540 acc 0.1292\n",
      "client 136 loss 3.1797 acc 0.1292\n",
      "client 137 loss 3.5010 acc 0.1292\n",
      "client 138 loss 2.1630 acc 0.1292\n",
      "client 139 loss 2.4997 acc 0.1292\n",
      "client 140 loss 2.0920 acc 0.1292\n",
      "client 141 loss 1.8948 acc 0.1292\n",
      "client 142 loss 2.5909 acc 0.1292\n",
      "client 143 loss 1.9505 acc 0.1292\n",
      "client 144 loss 2.4660 acc 0.1292\n",
      "client 145 loss 1.9876 acc 0.1292\n",
      "client 146 loss 1.7965 acc 0.1292\n",
      "client 147 loss 2.6238 acc 0.1292\n",
      "client 148 loss 2.8777 acc 0.1292\n",
      "client 149 loss 2.9162 acc 0.1292\n",
      "client 150 loss 2.2760 acc 0.1292\n",
      "client 151 loss 2.3964 acc 0.1292\n",
      "client 152 loss 2.9643 acc 0.1292\n",
      "client 153 loss 4.0263 acc 0.1292\n",
      "client 154 loss 3.4120 acc 0.1292\n",
      "client 155 loss 1.5876 acc 0.1292\n",
      "client 156 loss 2.6763 acc 0.1292\n",
      "client 157 loss 3.0272 acc 0.1292\n",
      "client 158 loss 3.0378 acc 0.1292\n",
      "client 159 loss 2.0057 acc 0.1292\n",
      "client 160 loss 2.9249 acc 0.1292\n",
      "client 161 loss 2.9908 acc 0.1292\n",
      "client 162 loss 3.0264 acc 0.1292\n",
      "client 163 loss 1.1885 acc 0.1292\n",
      "client 164 loss 3.2785 acc 0.1292\n",
      "client 165 loss 3.2359 acc 0.1292\n",
      "client 166 loss 2.8304 acc 0.1292\n",
      "client 167 loss 2.7614 acc 0.1292\n",
      "client 168 loss 3.6429 acc 0.1292\n",
      "client 169 loss 4.0934 acc 0.1292\n",
      "client 170 loss 2.9758 acc 0.1292\n",
      "client 171 loss 3.0328 acc 0.1292\n",
      "client 172 loss 3.0683 acc 0.1292\n",
      "client 173 loss 2.6930 acc 0.1292\n",
      "client 174 loss 3.9077 acc 0.1292\n",
      "client 175 loss 4.3254 acc 0.1292\n",
      "client 176 loss 2.9584 acc 0.1292\n",
      "client 177 loss 3.1935 acc 0.1292\n",
      "client 178 loss 1.9078 acc 0.1292\n",
      "client 179 loss 1.8465 acc 0.1292\n",
      "client 180 loss 1.9575 acc 0.1292\n",
      "client 181 loss 1.3547 acc 0.1292\n",
      "client 182 loss 3.9091 acc 0.1292\n",
      "client 183 loss 3.4954 acc 0.1292\n",
      "client 184 loss 2.5602 acc 0.1292\n",
      "client 185 loss 2.9545 acc 0.1292\n",
      "client 186 loss 1.8277 acc 0.1292\n",
      "client 187 loss 2.5986 acc 0.1292\n",
      "client 188 loss 3.1311 acc 0.1292\n",
      "client 189 loss 3.9404 acc 0.1292\n",
      "client 190 loss 4.2336 acc 0.1292\n",
      "client 191 loss 2.0248 acc 0.1292\n",
      "client 192 loss 3.9047 acc 0.1292\n",
      "client 193 loss 2.4957 acc 0.1292\n",
      "client 194 loss 3.4752 acc 0.1292\n",
      "client 195 loss 2.0396 acc 0.1292\n",
      "client 196 loss 3.2023 acc 0.1292\n",
      "client 197 loss 3.1578 acc 0.1292\n",
      "client 198 loss 2.9460 acc 0.1292\n",
      "client 199 loss 2.0960 acc 0.1292\n",
      "client 200 loss 1.8709 acc 0.1292\n",
      "client 201 loss 2.9240 acc 0.1292\n",
      "client 202 loss 2.9389 acc 0.1292\n",
      "client 203 loss 2.8718 acc 0.1292\n",
      "client 204 loss 1.7671 acc 0.1292\n",
      "client 205 loss 2.8987 acc 0.1292\n",
      "client 206 loss 2.5231 acc 0.1292\n",
      "client 207 loss 2.8043 acc 0.1292\n",
      "client 208 loss 4.2356 acc 0.1292\n",
      "client 209 loss 2.4791 acc 0.1292\n",
      "client 210 loss 2.6693 acc 0.1292\n",
      "client 211 loss 2.6563 acc 0.1292\n",
      "client 212 loss 2.5629 acc 0.1292\n",
      "client 213 loss 2.9018 acc 0.1292\n",
      "client 214 loss 3.0626 acc 0.1292\n",
      "client 215 loss 3.1654 acc 0.1292\n",
      "client 216 loss 2.3812 acc 0.1292\n",
      "client 217 loss 3.2915 acc 0.1292\n",
      "client 218 loss 2.4617 acc 0.1292\n",
      "client 219 loss 2.0680 acc 0.1292\n",
      "client 220 loss 2.3457 acc 0.1292\n",
      "client 221 loss 2.7092 acc 0.1292\n",
      "client 222 loss 1.8224 acc 0.1292\n",
      "client 223 loss 3.7552 acc 0.1292\n",
      "client 224 loss 3.2339 acc 0.1292\n",
      "client 225 loss 2.9284 acc 0.1292\n",
      "client 226 loss 3.5291 acc 0.1292\n",
      "client 227 loss 2.9627 acc 0.1292\n",
      "client 228 loss 2.9905 acc 0.1292\n",
      "client 229 loss 3.5618 acc 0.1292\n",
      "client 230 loss 2.1449 acc 0.1292\n",
      "client 231 loss 2.3203 acc 0.1292\n",
      "client 232 loss 4.4220 acc 0.1292\n",
      "client 233 loss 3.7473 acc 0.1292\n",
      "client 234 loss 2.2549 acc 0.1292\n",
      "client 235 loss 3.0437 acc 0.1292\n",
      "client 236 loss 2.8639 acc 0.1292\n",
      "client 237 loss 4.4637 acc 0.1292\n",
      "client 238 loss 1.4277 acc 0.1292\n",
      "client 239 loss 2.1181 acc 0.1292\n",
      "client 240 loss 3.0671 acc 0.1292\n",
      "client 241 loss 1.6982 acc 0.1292\n",
      "client 242 loss 2.4705 acc 0.1292\n",
      "client 243 loss 2.4097 acc 0.1292\n",
      "client 244 loss 2.4148 acc 0.1292\n",
      "client 245 loss 2.7208 acc 0.1292\n",
      "client 246 loss 2.8841 acc 0.1292\n",
      "client 247 loss 2.5936 acc 0.1292\n",
      "client 248 loss 3.7283 acc 0.1292\n",
      "client 249 loss 2.8358 acc 0.1292\n",
      "client 250 loss 2.9414 acc 0.1292\n",
      "client 251 loss 2.0273 acc 0.1292\n",
      "client 252 loss 2.4434 acc 0.1292\n",
      "client 253 loss 2.5841 acc 0.1292\n",
      "client 254 loss 2.4679 acc 0.1292\n",
      "client 255 loss 2.6453 acc 0.1292\n",
      "client 256 loss 2.9411 acc 0.1292\n",
      "client 257 loss 2.5801 acc 0.1292\n",
      "client 258 loss 3.0947 acc 0.1292\n",
      "client 259 loss 4.0630 acc 0.1292\n",
      "client 260 loss 4.5473 acc 0.1292\n",
      "client 261 loss 2.7957 acc 0.1292\n",
      "client 262 loss 2.0418 acc 0.1292\n",
      "client 263 loss 3.2655 acc 0.1292\n",
      "client 264 loss 3.0661 acc 0.1292\n",
      "client 265 loss 2.9768 acc 0.1292\n",
      "client 266 loss 2.3202 acc 0.1292\n",
      "client 267 loss 3.0589 acc 0.1292\n",
      "client 268 loss 4.1592 acc 0.1292\n",
      "client 269 loss 2.7834 acc 0.1292\n",
      "client 270 loss 3.3072 acc 0.1292\n",
      "client 271 loss 3.7168 acc 0.1292\n",
      "client 272 loss 2.1685 acc 0.1292\n",
      "client 273 loss 1.6799 acc 0.1292\n",
      "client 274 loss 2.0170 acc 0.1292\n",
      "client 275 loss 3.3575 acc 0.1292\n",
      "client 276 loss 2.6410 acc 0.1292\n",
      "client 277 loss 1.7702 acc 0.1292\n",
      "client 278 loss 2.3222 acc 0.1292\n",
      "client 279 loss 2.8007 acc 0.1292\n",
      "client 280 loss 3.0846 acc 0.1292\n",
      "client 281 loss 3.0693 acc 0.1292\n",
      "client 282 loss 2.9218 acc 0.1292\n",
      "client 283 loss 2.4898 acc 0.1292\n",
      "client 284 loss 2.7864 acc 0.1292\n",
      "client 285 loss 2.5979 acc 0.1292\n",
      "client 286 loss 2.7124 acc 0.1292\n",
      "client 287 loss 1.9801 acc 0.1292\n",
      "client 288 loss 1.3368 acc 0.1292\n",
      "client 289 loss 2.6158 acc 0.1292\n",
      "client 290 loss 3.2820 acc 0.1292\n",
      "client 291 loss 2.0647 acc 0.1292\n",
      "client 292 loss 2.4880 acc 0.1292\n",
      "client 293 loss 3.2184 acc 0.1292\n",
      "client 294 loss 3.3885 acc 0.1292\n",
      "client 295 loss 2.9494 acc 0.1292\n",
      "client 296 loss 3.0719 acc 0.1292\n",
      "client 297 loss 3.4188 acc 0.1292\n",
      "client 298 loss 4.1359 acc 0.1292\n",
      "client 299 loss 3.0112 acc 0.1292\n"
     ]
    }
   ],
   "source": [
    "# Get zeroth round loss, acc\n",
    "verbose = True\n",
    "if record[\"epoch\"] == 0:\n",
    "    logger.info(\"Logging initial loss, acc\")\n",
    "    client_losses = []\n",
    "    client_accs = []\n",
    "    for client_id in user_ids:\n",
    "        # Evaluate the client's model on the slice of the training data corresponding to the client's data\n",
    "        user_images = torch.from_numpy(dataset[\"train_data\"][\"images\"][dataset[\"user_with_data\"][client_id]]).to(config.device)\n",
    "        user_labels = torch.from_numpy(dataset[\"train_data\"][\"labels\"][dataset[\"user_with_data\"][client_id]]).to(config.device)\n",
    "        \n",
    "        # Get model outputs\n",
    "        output_on_own_data = model_dict[client_id](user_images)\n",
    "        output_on_test_set = model_dict[client_id](test_images)\n",
    "        \n",
    "        # Get losses/accs, and append to list\n",
    "        loss = loss_with_output(output_on_own_data, user_labels, config.loss)\n",
    "        acc = accuracy_with_output(output_on_test_set, test_labels)\n",
    "        \n",
    "        client_losses.append(loss)\n",
    "        client_accs.append(acc)\n",
    "        if verbose: logger.info(\"client {:d} loss {:.4f} acc {:.4f}\".format(client_id, loss, acc))\n",
    "    # Get rid of unnecessary variables to free up memory\n",
    "    del user_images; del user_labels; del output_on_own_data; del output_on_test_set\n",
    "    # Finally, append the initial losses, accs to the record\n",
    "    record[\"loss\"].append(client_losses)\n",
    "    record[\"testing_accuracy\"].append(client_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_client_dfedsam(user_model, user_id, dataset, config, logger, loss_fn, \n",
    "    local_epochs,\n",
    "    rho = 0.5,\n",
    "    adaptive = True,\n",
    "    lr = 0.1,\n",
    "    lr_decay = 0.998,\n",
    "    momentum = 0.5,\n",
    "    wd = 5e-4,\n",
    "    optimizer_batch_size = 32\n",
    "    ): \n",
    "    # Get data corresponding to a certain user\n",
    "    user_resource = assign_user_resource(config, user_id, \n",
    "                    dataset[\"train_data\"], dataset[\"user_with_data\"])\n",
    "\n",
    "    # Define cross-entropy criterion\n",
    "    loss_fn_pytorch = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define the SAM optimizer\n",
    "    base_optimizer = torch.optim.SGD\n",
    "    optimizer = SAM(user_model.parameters(), base_optimizer, rho=rho, adaptive=adaptive, \n",
    "        lr=lr* (lr_decay**comm_round), momentum=momentum, weight_decay=wd)\n",
    "\n",
    "    # Dataset stuff\n",
    "    np_dataset = NumpyDataset(user_resource[\"images\"], user_resource[\"labels\"], transform=numpy_to_tensor_transform)\n",
    "    user_data_loader = DataLoader(np_dataset, batch_size=optimizer_batch_size, shuffle=True)\n",
    "\n",
    "    # Doing local_epochs number of local training rounds\n",
    "    for epoch in range(local_epochs):\n",
    "        # Iterate over the user's data\n",
    "        epoch_loss, epoch_acc = [], []\n",
    "        for batch_idx, (x, labels) in enumerate(user_data_loader):\n",
    "            x, labels = x.to(config.device), labels.to(config.device)\n",
    "\n",
    "            # From the SAM codebase\n",
    "            # first forward-backward step\n",
    "            pred = user_model(x)\n",
    "            \n",
    "            # Don't need enable_running_stats due to no batchnorm or any moving averages\n",
    "            # enable_running_stats(model)\n",
    "            # log_probs = model.forward(x)\n",
    "            # loss = loss_fn_pytorch(user_model(x), labels.long())\n",
    "            loss = loss_fn_pytorch(user_model(x), labels)\n",
    "            loss.backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "\n",
    "            # second forward-backward step\n",
    "            # Don't need disable_running_stats due to no batchnorm or any moving averages\n",
    "            # disable_running_stats(model)\n",
    "            # loss_fn_pytorch(user_model(x), labels.long()).backward()\n",
    "            loss_fn_pytorch(user_model(x), labels).backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "            \n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "        if config.verbose: \n",
    "            print('Client Index = {}\\tEpoch: {}\\tLoss: {:.6f}'.format(\n",
    "            user_id, epoch, sum(epoch_loss) / len(epoch_loss)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comm Round: 0\n",
      "Round 0: Test Loss: 0.40996190905570984, Avg Client Acc: 0.8256143110990525, Agg Acc: 0.8858000040054321\n"
     ]
    }
   ],
   "source": [
    "# for comm_round in range(record[\"epoch\"],record[\"epoch\"]+config.rounds):\n",
    "for comm_round in range(1):\n",
    "    logger.info(f\"Comm Round: {comm_round}\")\n",
    "    client_losses = []\n",
    "    client_accs = []\n",
    "\n",
    "    # Create the graph for this round\n",
    "    if config.topology == \"random\":\n",
    "        G = create_random_graph(config.users, config.p, config.graph_name)\n",
    "    elif config.topology == \"ring\":\n",
    "        G = create_ring_graph(config.users, config.graph_name)\n",
    "    elif config.topology == \"regular\":\n",
    "        if config.p is not None:\n",
    "            raise ValueError(\"Regular graph requires d, not p\")\n",
    "        elif config.d is None:\n",
    "            raise ValueError(\"Regular graph requires d\")\n",
    "        G = create_regular_graph(config.users, config.d, config.graph_name)\n",
    "        if config.verbose: logger.info(f\"Creating regular graph with d={config.d}\")\n",
    "    else: \n",
    "        raise ValueError(\"Invalid topology: {}\".format(config.topology))\n",
    "\n",
    "    for client_id in user_ids:\n",
    "        # SGD w/ momentum\n",
    "        loss_fn_pytorch = nn.CrossEntropyLoss()\n",
    "        train_client_dfedsam(model_dict[client_id], client_id, dataset, config, logger, loss_fn_pytorch, \n",
    "        # Dfedsam parameters\n",
    "        rho = 0.5,\n",
    "        adaptive = True,\n",
    "        lr = 0.1,\n",
    "        lr_decay = 0.998,\n",
    "        momentum = 0.5,\n",
    "        wd = 5e-4,\n",
    "        optimizer_batch_size = 32)\n",
    "\n",
    "    # All clients average with neighbors\n",
    "    # Must find the avged weights then load weights to mimic synchronous averaging\n",
    "    new_avged_weights = {}\n",
    "    # Get new weights for all clients\n",
    "    for client_id in user_ids:\n",
    "        neighbors = list(G.neighbors(client_id))\n",
    "        new_avged_weights[client_id] = average_neighbor_weights(client_id, neighbors, model_dict)\n",
    "    # Load new weights for all clients\n",
    "    for client_id in user_ids:\n",
    "        model_dict[client_id].load_state_dict(new_avged_weights[client_id])\n",
    "    del new_avged_weights\n",
    "    \n",
    "    # Now, test individual client accs\n",
    "    with torch.no_grad():\n",
    "        for client_id in user_ids:\n",
    "            # Get client accuracy\n",
    "            output_on_test_set = model_dict[client_id](test_images)\n",
    "            acc = accuracy_with_output(output_on_test_set, test_labels)\n",
    "            client_accs.append(acc)\n",
    "\n",
    "    # Test the global, aggregated model\n",
    "    # Note: Weighted averaging is unnecessary since all clients have the same number of samples\n",
    "    \n",
    "    # Init model to load aggregated state dict\n",
    "    temp_global_model = init_model(config, logger)\n",
    "    temp_global_model.load_state_dict(average_neighbor_weights(0, user_ids[1:], model_dict))\n",
    "    \n",
    "    global_output = temp_global_model(test_images)\n",
    "    global_loss = nn.CrossEntropyLoss()(global_output, test_labels)\n",
    "    global_acc = accuracy_with_output(global_output, test_labels)\n",
    "    \n",
    "    if comm_round % 5 == 0: \n",
    "        logger.info(f\"Round {comm_round}: Test Loss: {global_loss.item()}, Avg Client Acc: {np.mean(client_accs)}, Agg Acc: {global_acc}\")\n",
    "\n",
    "    # Record the results\n",
    "    record[\"testing_accuracy\"].append(client_accs)\n",
    "    \n",
    "    if 'aggregated_accs' in record:\n",
    "        record['aggregated_accs'].append(global_acc)\n",
    "    else:\n",
    "        record['aggregated_accs'] = [global_acc]\n",
    "\n",
    "    record[\"epoch\"] += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I write the local training function that takes INPUTS\n",
    "- user_model\n",
    "- user_id\n",
    "- dataset\n",
    "- config\n",
    "- logger\n",
    "- loss_fn \n",
    "- other optimizer parameters (IMPORTANT FOR DFEDSAM comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input variables corresponding to a certain user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = 0\n",
    "user_model = model_dict[user_id]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffusionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
