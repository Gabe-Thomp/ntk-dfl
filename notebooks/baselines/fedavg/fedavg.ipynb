{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FedAvg Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "directory_path = \"../../../\"\n",
    "if directory_path not in sys.path:\n",
    "    # Add the directory to sys.path\n",
    "    sys.path.append(directory_path)\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils import data\n",
    "from torch import optim\n",
    "\n",
    "from utils.utils import *\n",
    "from utils import load_config\n",
    "from utils.validate import *\n",
    "from fedlearning.model import *\n",
    "from fedlearning.dataset import *\n",
    "from fedlearning.evolve import *\n",
    "from fedlearning.optimizer import GlobalUpdater, LocalUpdater, get_omegas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Loaded configuration from configs/config_fedavg.yaml\n",
      "Dataset path: ../../data/fmnist/train.dat\n",
      "IID Dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating config from filepath:  /home/gathomp3/Deep_Learning/NeuralTangent/ntk-fed/notebooks/baselines/../../utils\n",
      "/home/gathomp3/Deep_Learning/NeuralTangent/ntk-fed/notebooks/baselines/../../../records/baseline_trials/fedavg/trial1/train.log\n"
     ]
    }
   ],
   "source": [
    "config_file = \"baseline_configs/config_fedavg.yaml\"\n",
    "config = load_config(config_file)\n",
    "\n",
    "logger = init_logger(config)\n",
    "logger.info(\"Loaded configuration from {}\".format(config_file))\n",
    "logger.info(\"Dataset path: {}\".format(config.train_data_dir))\n",
    "\n",
    "if config.user_with_data == \"\":\n",
    "    logger.info(\"IID Dataset\")\n",
    "else:\n",
    "    logger.info(f\"Using \\\"{config.user_with_data}\\\" premade Non-IID dataset\")\n",
    "\n",
    "# Define a model to extract number of parameters for record\n",
    "if config.record_path is not None:\n",
    "    record = load_record(config.record_path)\n",
    "    logger.info(\"Loaded record from {}\".format(config.record_path))\n",
    "    loaded_record = True\n",
    "else:\n",
    "    model = init_model(config, logger)\n",
    "    record = init_record(config, model)\n",
    "    loaded_record = False\n",
    "\n",
    "if config.device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user_ids\n",
    "user_ids = np.arange(0, config.users)\n",
    "# load the dataset\n",
    "# dataset object is a dictionary with keys: train_data, test_data, user_with_data\n",
    "# user_with_data is a dictionary with keys: userID:sampleID\n",
    "# For example, in the IID setting ID's are just assigned like 0, 1, 2, 3, ...\n",
    "dataset = assign_user_data(config, logger)\n",
    "test_images = torch.from_numpy(dataset[\"test_data\"][\"images\"]).to(config.device)\n",
    "test_labels = torch.from_numpy(dataset[\"test_data\"][\"labels\"]).to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary for optimization with pytorch\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy array): Array of data samples.\n",
    "            targets (numpy array): Array of labels corresponding to the data samples.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, target\n",
    "\n",
    "def numpy_to_tensor_transform(data):\n",
    "    return torch.from_numpy(data)\n",
    "\n",
    "def train_on_client_data(user_id, global_model, dataset, config, logger, \n",
    "local_update_steps, local_bs, lr, loss_fn = \"ce\", verbose=False):\n",
    "    # Create a copy of the global model to be used for training\n",
    "    user_model = copy.deepcopy(global_model)\n",
    "    \n",
    "    # Get data corresponding to a certain user\n",
    "    user_resource = assign_user_resource(config, user_id, \n",
    "                        dataset[\"train_data\"], dataset[\"user_with_data\"])\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.SGD(user_model.parameters(), lr=lr)\n",
    "    \n",
    "    # Define the dataset\n",
    "    np_dataset = NumpyDataset(user_resource[\"images\"], user_resource[\"labels\"], transform=numpy_to_tensor_transform)\n",
    "    \n",
    "    # Define the dataloader\n",
    "    user_data_loader = DataLoader(np_dataset, batch_size=local_bs, shuffle=True)\n",
    "\n",
    "    # Define the loss function\n",
    "    if loss_fn == \"ce\": \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    else: \n",
    "        raise ValueError(\"Loss function not implemented\")\n",
    "\n",
    "    for local_epoch in range(local_update_steps):\n",
    "         # Iterate over the user's data\n",
    "        for batch_idx, (data, target) in enumerate(user_data_loader):\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = user_model(data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0 and verbose: \n",
    "                logger.info(f'Train Epoch: {local_epoch} [{batch_idx * len(data)}/{len(user_data_loader.dataset)} ({100. * batch_idx / len(user_data_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    if verbose: print()\n",
    "    \n",
    "    return user_model\n",
    "\n",
    "def average_neighbor_weights(client_id, neighbor_ids, model_dict):\n",
    "    # Average the weights of the models in the cluster\n",
    "    weight_dict = copy.deepcopy(model_dict[client_id].state_dict())\n",
    "    weight_aggregator = WeightMod(weight_dict)\n",
    "    for user_id in neighbor_ids:\n",
    "        weight_aggregator.add(copy.deepcopy(model_dict[user_id].state_dict()))\n",
    "    # Add one for the client itself\n",
    "    weight_aggregator.mul(1.0/ (len(neighbor_ids)+1) )\n",
    "    return weight_aggregator.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fed Avg Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Client Fraction\n",
    "# C = 1.0\n",
    "\n",
    "# # Number of rounds\n",
    "# comm_rounds = 100\n",
    "\n",
    "# # Number of local updates\n",
    "# local_update_steps = 5\n",
    "\n",
    "# # Batch size\n",
    "# local_bs = 100\n",
    "\n",
    "# # Learning rate\n",
    "# lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gathomp3/anaconda3/envs/DiffusionEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Round 0: Loss: 1.2307368516921997, Accuracy: 0.6110000014305115\n",
      "Round 1: Loss: 0.9845389127731323, Accuracy: 0.6807000041007996\n"
     ]
    }
   ],
   "source": [
    "global_model = init_model(config, logger)\n",
    "\n",
    "for comm_round in range(config.rounds):\n",
    "    # Empty model dict to store the client updated models\n",
    "    temp_model_dict = {}\n",
    "\n",
    "    # Select C fraction of clients randomly\n",
    "    participating_client_ids = np.random.choice(user_ids, int(config.part_rate * config.users), replace=False)\n",
    "    \n",
    "    # Train on all participating clients\n",
    "    for client_id in participating_client_ids:\n",
    "        # Train on the client's data\n",
    "        user_model = train_on_client_data(client_id, global_model, dataset, config, logger, \n",
    "                                        config.local_update_steps, config.sgd_batch_size, config.lr, \n",
    "                                        loss_fn = config.loss, verbose=config.verbose)\n",
    "        temp_model_dict[client_id] = user_model\n",
    "    \n",
    "    # Average the deviated weights\n",
    "    averaged_state_dict = average_neighbor_weights(participating_client_ids[0], participating_client_ids[1:], temp_model_dict)\n",
    "    \n",
    "    # Load the averaged weights to the global model\n",
    "    global_model.load_state_dict(averaged_state_dict)\n",
    "\n",
    "    # Test the global model\n",
    "    output = global_model(test_images)\n",
    "    loss = nn.CrossEntropyLoss()(output, test_labels)\n",
    "    acc = accuracy_with_output(output, test_labels)\n",
    "    logger.info(f\"Round {comm_round}: Loss: {loss.item()}, Accuracy: {acc}\")\n",
    "\n",
    "    # Record the results\n",
    "    record[\"loss\"].append(loss.item())\n",
    "    record[\"testing_accuracy\"].append(acc)\n",
    "    record[\"epoch\"] += 1\n",
    "\n",
    "# Save the record\n",
    "record[\"global_model\"] = global_model.state_dict()\n",
    "record[\"fedavg_hyperparameters\"] = {\"learning_rate\": config.lr, \"local_update_steps\": config.local_update_steps,\n",
    "                                    \"sgd_batch_size\": config.sgd_batch_size, \"participation_rate\": config.part_rate}  \n",
    "record[\"user_with_data\"] = config.user_with_data\n",
    "# save_record(config, record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_parameters': 79510,\n",
       " 'batch_size': 200,\n",
       " 'lr': 0.01,\n",
       " 'taus': [],\n",
       " 'testing_accuracy': [0.6110000014305115, 0.6807000041007996],\n",
       " 'loss': [1.2307368516921997, 0.9845389127731323],\n",
       " 'rounds': 2,\n",
       " 'iid': True,\n",
       " 'model_accs_per_round': [],\n",
       " 'model_num_training_rounds': [],\n",
       " 'epoch': 2,\n",
       " 'global_model': OrderedDict([('fc1.weight',\n",
       "               tensor([[ 3.6875e-06,  5.1048e-02, -1.6980e-02,  ...,  3.0329e-02,\n",
       "                         5.5418e-02,  2.2462e-02],\n",
       "                       [-6.0521e-02,  6.4522e-02, -3.0663e-02,  ...,  3.0762e-02,\n",
       "                         6.1197e-03, -4.9456e-02],\n",
       "                       [ 2.8096e-02, -7.5576e-02,  6.5789e-02,  ..., -5.0729e-02,\n",
       "                         1.4562e-02,  1.8989e-02],\n",
       "                       ...,\n",
       "                       [ 1.7037e-04, -1.7807e-03,  6.5987e-02,  ...,  3.5773e-02,\n",
       "                         3.1405e-02, -1.1933e-02],\n",
       "                       [ 4.2800e-03, -7.2544e-03,  2.3580e-02,  ..., -8.5815e-02,\n",
       "                        -9.5069e-02,  1.9961e-02],\n",
       "                       [-1.9701e-02,  4.0868e-03,  1.8887e-02,  ...,  2.6768e-02,\n",
       "                         2.7124e-02, -1.9524e-02]], device='cuda:0')),\n",
       "              ('fc1.bias',\n",
       "               tensor([-3.3729e-03,  1.1544e-03,  3.3105e-06, -2.8705e-04, -1.0979e-03,\n",
       "                        7.1016e-04, -3.3203e-04,  8.9286e-04, -1.0262e-03,  1.5852e-04,\n",
       "                       -2.1435e-03,  4.6736e-04, -5.7029e-04,  1.2488e-03, -1.6950e-03,\n",
       "                        4.5198e-03,  3.8402e-04,  2.1769e-03,  5.2972e-04, -1.3015e-03,\n",
       "                        3.1933e-04,  3.0457e-04, -2.9593e-03, -5.6905e-04, -7.0838e-04,\n",
       "                       -6.2835e-04, -2.4779e-03, -3.9395e-03, -2.2907e-03,  7.7818e-05,\n",
       "                        1.1239e-03,  6.0207e-05,  4.1642e-03, -1.0569e-03,  1.9297e-03,\n",
       "                       -6.5199e-04, -4.3955e-03,  1.5218e-03,  1.0055e-03,  1.3245e-03,\n",
       "                       -8.2862e-04, -2.1384e-04,  1.3701e-03, -5.2493e-04, -1.6450e-03,\n",
       "                        1.7602e-03,  1.0252e-03,  3.4975e-03,  2.5499e-03, -1.1978e-03,\n",
       "                       -1.7545e-05,  4.8852e-03,  2.7635e-03,  2.0871e-03,  2.3648e-03,\n",
       "                       -2.6501e-03,  1.3808e-03,  1.6152e-04,  8.4877e-04, -1.7375e-03,\n",
       "                        3.2518e-03, -2.5528e-04,  1.0711e-04,  2.2153e-04, -1.0164e-03,\n",
       "                        3.7456e-05, -9.9699e-04, -3.1537e-03, -1.0939e-03, -4.6883e-03,\n",
       "                        1.8721e-03,  2.6825e-04, -8.3262e-05, -3.3385e-03, -1.6836e-03,\n",
       "                       -9.6655e-04,  1.9900e-04, -2.1283e-04, -4.5993e-05,  8.7830e-04,\n",
       "                        3.0985e-04,  3.4921e-03, -3.1363e-03, -2.0219e-03,  1.1024e-03,\n",
       "                        4.5619e-03,  8.5604e-04,  2.4492e-04, -3.1715e-03, -2.9991e-03,\n",
       "                       -3.0188e-03,  5.9808e-04,  3.4883e-03, -1.4044e-03,  1.6714e-03,\n",
       "                        4.3773e-04,  2.4953e-03,  1.3286e-03, -2.1800e-03, -4.6600e-04],\n",
       "                      device='cuda:0')),\n",
       "              ('fc2.weight',\n",
       "               tensor([[ 2.0213e-01, -9.8896e-02, -3.8186e-01, -1.0171e-01, -3.7699e-02,\n",
       "                        -1.7509e-01,  7.4534e-02,  8.2355e-02, -1.8244e-01, -5.9185e-02,\n",
       "                         1.0241e-01,  2.3836e-02, -1.1123e-02, -4.8305e-02,  1.7694e-03,\n",
       "                        -1.5202e-01,  1.5012e-01, -4.0851e-02,  1.3789e-01,  1.6425e-01,\n",
       "                        -1.2108e-01,  1.3035e-01, -2.2250e-02,  6.8378e-02, -1.0837e-01,\n",
       "                        -6.6483e-02,  1.6045e-01, -1.2512e-03,  1.2621e-01, -1.3482e-01,\n",
       "                         2.9930e-02, -8.2320e-03, -1.3674e-02, -2.3781e-01,  8.2545e-02,\n",
       "                         4.9184e-02,  3.2337e-02,  3.1406e-01,  1.2081e-01,  5.4193e-02,\n",
       "                         6.7171e-02,  1.3472e-01, -7.9715e-02, -2.1966e-02, -1.5718e-01,\n",
       "                         2.0815e-01, -5.7214e-02,  2.7519e-01, -2.1994e-01,  5.8350e-02,\n",
       "                         1.1966e-01, -1.3449e-01, -6.5701e-02,  2.1295e-01,  2.0005e-01,\n",
       "                         2.9617e-02, -2.2370e-02,  2.8650e-01, -1.4912e-01, -5.9279e-02,\n",
       "                        -4.2650e-02, -9.7200e-02, -1.4850e-01, -2.3795e-01,  1.3133e-01,\n",
       "                         8.7161e-02,  2.7155e-02,  4.2320e-01,  1.4758e-01, -3.8456e-02,\n",
       "                        -1.6576e-02, -1.3163e-02, -2.7920e-01,  1.8033e-01,  2.2206e-02,\n",
       "                         1.7735e-01,  9.7237e-02, -1.2717e-01,  2.1378e-01, -2.7349e-01,\n",
       "                        -6.1284e-02,  2.5903e-01,  1.5831e-01,  6.7808e-02,  1.9357e-01,\n",
       "                        -4.2095e-02, -1.3401e-01,  1.2554e-01,  1.2481e-02,  3.7926e-02,\n",
       "                        -3.6308e-02,  6.4828e-02, -2.7361e-01,  1.0316e-02,  2.4497e-01,\n",
       "                         1.2341e-01,  5.5638e-03,  4.3375e-01, -4.8767e-03,  1.5755e-01],\n",
       "                       [-8.0634e-02, -2.4243e-01, -9.3190e-02,  2.8793e-02,  5.7839e-03,\n",
       "                         1.4110e-01,  7.2922e-02,  4.2239e-02,  1.1235e-01, -4.5968e-02,\n",
       "                         7.2103e-02, -7.4764e-03,  1.4051e-01, -6.0839e-02,  1.2194e-01,\n",
       "                        -4.7993e-02, -4.4946e-03,  4.6084e-02, -1.9160e-01, -7.7395e-02,\n",
       "                         4.6852e-02,  8.3915e-02,  1.4400e-02,  1.8845e-01, -7.0182e-02,\n",
       "                         1.3566e-01, -1.1711e-01,  2.0622e-01, -3.3319e-01,  3.9209e-02,\n",
       "                         9.0907e-02,  8.3834e-02, -3.9087e-02,  5.9207e-02,  1.7065e-03,\n",
       "                        -7.9420e-03,  2.4125e-01, -7.7247e-03,  4.0011e-02, -1.4076e-01,\n",
       "                         2.0539e-03, -8.6699e-02, -4.7921e-02,  5.5813e-02, -1.7506e-02,\n",
       "                         1.9369e-01,  3.7213e-02,  7.5962e-02, -1.1383e-01, -5.4310e-02,\n",
       "                         1.2814e-01, -1.7546e-01, -6.5691e-02,  1.6647e-01,  1.5515e-01,\n",
       "                        -1.3489e-01,  1.6562e-01, -7.3857e-03,  3.2430e-03,  4.4852e-02,\n",
       "                        -1.0699e-01,  1.8313e-03, -6.1869e-03, -1.8753e-01,  2.0149e-01,\n",
       "                         1.3226e-01, -1.3292e-01,  2.8519e-01, -1.2307e-02, -1.0338e-01,\n",
       "                        -1.0998e-01, -1.0934e-03,  5.7180e-02,  1.0607e-01,  2.6203e-01,\n",
       "                         3.2477e-01,  3.3880e-01,  9.8814e-02, -2.0971e-01, -2.0165e-01,\n",
       "                        -1.3598e-01, -8.2967e-02,  1.7958e-01,  5.1137e-02, -2.2210e-01,\n",
       "                         1.0324e-02,  1.3807e-02, -8.2769e-02, -8.2801e-02, -5.7899e-02,\n",
       "                         5.5350e-03,  1.4842e-01, -2.6744e-01,  2.1035e-01, -1.0786e-01,\n",
       "                        -1.9547e-01, -3.6826e-01,  1.6798e-01,  2.7804e-01,  3.3973e-01],\n",
       "                       [-2.5098e-01, -1.1700e-01, -1.6854e-01,  6.4350e-02, -1.0755e-01,\n",
       "                         1.5551e-01, -9.9800e-02, -1.1282e-02,  6.9777e-02, -1.2059e-01,\n",
       "                        -2.3608e-01,  6.0020e-03,  8.8590e-02, -6.9042e-02,  1.2338e-02,\n",
       "                         5.6099e-02,  4.4398e-02, -2.1298e-01, -1.2413e-01,  1.4347e-02,\n",
       "                         2.6296e-01, -5.8994e-02, -1.4646e-01, -1.6732e-01,  3.3705e-02,\n",
       "                         1.4572e-01,  5.6507e-02, -3.0297e-02, -1.3017e-01, -3.0931e-02,\n",
       "                         1.8237e-01, -1.6842e-01,  2.4328e-01, -6.8768e-02, -1.5061e-01,\n",
       "                        -7.6996e-03, -1.1775e-02, -3.8823e-02, -1.9074e-01,  1.9045e-01,\n",
       "                        -1.6756e-02, -1.2735e-02, -2.8578e-02,  2.1519e-01, -1.1296e-01,\n",
       "                         6.9209e-03,  8.7694e-02, -1.7263e-01,  1.3835e-01, -1.5724e-01,\n",
       "                         9.2655e-02,  7.8750e-02,  1.3260e-01,  1.6628e-01,  2.5425e-01,\n",
       "                        -2.0838e-01, -1.6726e-01,  8.3162e-02, -9.5236e-02, -5.3866e-02,\n",
       "                         1.8945e-01,  1.1136e-01, -1.5218e-01, -7.4736e-02, -7.5014e-03,\n",
       "                         2.2055e-01,  2.1945e-02, -1.2259e-01, -6.7005e-02, -3.0682e-01,\n",
       "                        -9.3435e-02,  4.7447e-02,  1.2016e-01, -2.1183e-01, -2.7470e-01,\n",
       "                         1.8256e-01, -1.6465e-02,  1.5040e-01, -9.5376e-02,  1.1791e-01,\n",
       "                        -1.1294e-02,  1.8925e-01, -1.5975e-02, -3.6256e-02, -7.6541e-02,\n",
       "                        -1.2457e-01,  1.0622e-01, -8.2250e-02,  3.8697e-02, -4.2504e-01,\n",
       "                        -2.2928e-01, -1.0772e-02,  3.2419e-02,  2.8476e-03, -9.6778e-03,\n",
       "                        -2.6452e-01, -1.3051e-01, -7.9932e-02, -4.8166e-02, -1.7142e-03],\n",
       "                       [ 7.8708e-02, -1.3541e-02, -1.9389e-01, -6.6408e-02, -1.0487e-02,\n",
       "                        -9.9255e-02, -1.6598e-01, -1.4458e-02,  1.9698e-01, -2.8067e-01,\n",
       "                        -4.9804e-02, -6.2096e-02,  1.5956e-01, -5.7062e-02,  1.8417e-02,\n",
       "                        -2.9432e-02,  1.0941e-01,  1.9287e-02, -4.9376e-02,  8.0278e-02,\n",
       "                         7.7195e-02,  3.1214e-02,  1.6160e-02,  5.2199e-02,  1.1674e-01,\n",
       "                        -2.8103e-02, -2.7113e-01, -1.6708e-01, -9.4944e-02, -1.5898e-01,\n",
       "                        -4.1367e-03,  9.2363e-02,  1.7818e-02,  6.1363e-02,  1.1272e-01,\n",
       "                        -6.3075e-02, -1.0245e-01, -1.7914e-01,  2.3437e-01,  7.5850e-02,\n",
       "                        -1.7739e-01, -2.1258e-01,  5.6231e-02, -7.4632e-02, -1.6849e-01,\n",
       "                         1.8857e-01,  1.1692e-01, -2.2199e-01,  6.2527e-02,  3.2696e-01,\n",
       "                         1.9160e-02, -1.7915e-01,  4.0317e-03,  5.7807e-02, -2.2735e-02,\n",
       "                        -7.7350e-02, -1.0835e-01, -8.6527e-04, -7.0801e-02, -1.1471e-01,\n",
       "                         1.2484e-01, -5.9169e-02,  1.9250e-01,  5.8652e-02,  3.6083e-02,\n",
       "                         1.7675e-03, -4.5789e-02, -9.6345e-02,  5.4168e-02,  2.3543e-02,\n",
       "                        -9.6567e-02,  1.2797e-01,  1.2301e-01, -9.7135e-02, -1.1917e-02,\n",
       "                        -1.6270e-01,  1.2260e-01,  2.3189e-02, -1.4968e-01,  1.1390e-02,\n",
       "                        -2.9645e-01, -5.2972e-02, -1.3316e-01,  2.0707e-01,  1.2565e-01,\n",
       "                        -5.7087e-02,  1.9782e-01, -9.2166e-02, -3.4209e-02, -1.7697e-01,\n",
       "                        -1.0754e-01,  1.0042e-02, -2.7081e-01, -2.5049e-01,  6.6190e-02,\n",
       "                        -2.2653e-01, -7.2357e-02, -6.8385e-02,  6.9505e-02,  8.8231e-02],\n",
       "                       [ 1.6354e-01, -2.0157e-01, -1.1805e-01,  1.8581e-01,  2.6162e-01,\n",
       "                         1.6705e-01, -3.9982e-02, -2.5789e-01, -5.0618e-02,  3.1896e-03,\n",
       "                        -2.4583e-01,  2.4370e-02, -4.3310e-02, -1.2871e-01, -4.0049e-02,\n",
       "                         8.4632e-02,  1.0241e-01, -2.6873e-03,  2.3469e-02, -1.8427e-01,\n",
       "                         1.4223e-01, -2.6911e-01,  3.6039e-01, -5.1450e-02,  5.0482e-02,\n",
       "                         5.7111e-02,  2.3656e-02,  1.3475e-01,  1.6424e-01, -1.7985e-02,\n",
       "                        -1.4022e-02,  1.3825e-01,  8.8956e-02, -1.1720e-01,  4.0177e-02,\n",
       "                         2.1421e-01, -4.3319e-02, -1.1439e-01, -1.5228e-01, -2.0994e-01,\n",
       "                         1.0698e-01, -4.8755e-03,  4.7433e-03, -6.2324e-02, -1.3484e-01,\n",
       "                        -1.1049e-01,  2.4571e-02, -1.7937e-01,  4.0902e-02,  2.4930e-01,\n",
       "                         7.3019e-02, -1.9079e-02,  8.1032e-02, -3.8895e-01, -2.7276e-01,\n",
       "                        -2.2828e-01, -3.2193e-02, -3.2132e-02, -1.4201e-01,  1.6342e-01,\n",
       "                        -7.6584e-02,  2.0684e-02, -4.3079e-02, -2.1402e-01, -8.8953e-02,\n",
       "                        -1.6782e-01, -8.3909e-02,  8.5029e-02,  8.9475e-02,  5.1576e-02,\n",
       "                         1.5144e-01,  1.9971e-01,  1.3318e-02,  4.7585e-01,  7.2376e-02,\n",
       "                         1.3247e-01,  6.9389e-02,  2.5865e-01, -2.9241e-01, -2.9103e-01,\n",
       "                         1.2201e-01, -2.2060e-02, -9.7268e-02,  5.3160e-02,  5.5927e-02,\n",
       "                         2.8760e-02,  1.2018e-02,  3.5237e-02,  1.8889e-01,  1.3610e-01,\n",
       "                        -1.0209e-01, -1.4200e-01,  2.0357e-02, -2.0158e-02, -1.6944e-01,\n",
       "                         5.5657e-03, -9.2493e-02, -4.9544e-02, -2.2723e-01,  2.3462e-01],\n",
       "                       [-1.1254e-01, -1.0093e-01, -4.4870e-02,  6.0231e-02,  1.1161e-01,\n",
       "                         5.3420e-02,  1.7864e-01, -1.1352e-01,  2.6136e-04,  2.6146e-01,\n",
       "                         1.1135e-01, -1.2795e-01, -8.3995e-02,  1.0508e-01,  1.5044e-01,\n",
       "                         2.5420e-01,  1.7144e-02,  8.8626e-02,  1.3505e-01, -2.0171e-01,\n",
       "                        -3.0647e-01, -8.1173e-02, -8.1724e-02,  1.2607e-01,  6.5941e-03,\n",
       "                         1.5404e-01,  4.0590e-02, -2.3183e-02, -1.2539e-01,  1.3000e-01,\n",
       "                         3.7568e-02,  7.6249e-02,  2.1629e-02, -2.6608e-01,  3.6653e-01,\n",
       "                        -2.4377e-01,  3.0351e-02,  7.9032e-02,  1.8403e-01, -1.3512e-02,\n",
       "                        -2.8643e-02, -1.4986e-01,  1.4994e-02, -4.6754e-03,  2.2227e-02,\n",
       "                        -8.8546e-02,  2.2702e-01,  1.0283e-01,  6.1160e-02,  4.5425e-02,\n",
       "                        -2.6620e-01,  1.3484e-02, -3.4802e-02, -1.0141e-01,  2.5597e-01,\n",
       "                        -8.7840e-02,  2.0775e-01,  3.2640e-01,  2.8415e-01,  3.9075e-03,\n",
       "                         2.3759e-01,  2.7356e-02,  9.8031e-02, -4.6085e-02, -1.0227e-01,\n",
       "                         4.9633e-02, -1.0482e-01, -1.7526e-01,  4.5590e-03, -1.5145e-01,\n",
       "                         2.0492e-01,  2.8542e-02,  4.8172e-02, -1.2914e-01,  1.3507e-01,\n",
       "                        -1.1079e-01,  6.4062e-02,  2.9452e-03, -4.9128e-02, -7.9347e-02,\n",
       "                         2.0134e-01, -1.6664e-02,  1.2516e-01, -4.8619e-02,  5.6752e-02,\n",
       "                         2.7340e-01,  5.5528e-02,  4.4426e-02,  1.8645e-02, -2.9554e-03,\n",
       "                        -2.2487e-01,  1.0886e-02,  1.1568e-01,  1.1206e-02, -3.9498e-02,\n",
       "                         1.6088e-01, -1.6479e-01, -4.8546e-02, -1.2012e-02,  3.2982e-02],\n",
       "                       [-8.3335e-02,  2.7647e-01,  9.2263e-02, -3.0255e-02,  2.1816e-01,\n",
       "                        -1.6609e-01, -2.4183e-02,  1.9043e-01,  1.8349e-03,  8.3537e-02,\n",
       "                         1.2009e-01,  2.8642e-01, -2.0316e-01, -2.1026e-01,  6.1125e-02,\n",
       "                        -1.1107e-01, -8.0933e-02,  1.6170e-02,  1.5185e-01, -3.4641e-02,\n",
       "                        -2.7601e-01, -2.1509e-01, -8.3184e-02, -3.0086e-01, -1.4471e-01,\n",
       "                         4.1498e-02, -2.0739e-01, -1.6571e-02,  1.4819e-01,  1.0059e-01,\n",
       "                        -2.4366e-01, -7.6903e-03,  1.3614e-01, -1.3999e-01,  1.2284e-01,\n",
       "                        -5.1398e-02, -1.3473e-01,  3.6578e-02, -9.7117e-02, -1.1761e-02,\n",
       "                         1.6441e-01,  9.5634e-02,  8.4403e-02, -6.3223e-02,  6.9826e-02,\n",
       "                         2.0838e-01, -2.9909e-02,  2.4109e-01, -3.6296e-02,  1.7688e-01,\n",
       "                         2.4407e-01,  1.4164e-02, -1.2595e-02, -2.8047e-02,  8.2502e-02,\n",
       "                         3.2501e-02, -2.0049e-01,  1.0014e-01,  3.9399e-01,  2.8856e-01,\n",
       "                         1.3756e-01,  7.2487e-02,  8.1405e-02,  1.0098e-01, -2.2856e-01,\n",
       "                        -2.2181e-02,  1.0983e-01, -7.3585e-02,  4.3906e-01, -2.2293e-01,\n",
       "                         2.2268e-01,  4.8002e-02,  2.7942e-01, -4.5258e-02,  2.3117e-01,\n",
       "                         1.4938e-02,  1.2017e-01,  1.2756e-01, -1.3295e-01, -1.0815e-01,\n",
       "                         3.0881e-02,  4.7244e-03,  4.8925e-03, -1.2243e-01,  1.4071e-01,\n",
       "                        -7.8605e-02, -6.7948e-02,  4.5740e-04, -3.6553e-02,  4.4602e-02,\n",
       "                        -4.1326e-02, -8.1130e-02, -3.1831e-02,  1.6448e-01,  1.4896e-01,\n",
       "                        -4.2239e-02, -2.4629e-01,  2.9684e-01, -9.1289e-02, -2.0144e-01],\n",
       "                       [-6.7766e-02,  1.8035e-01, -2.4331e-02, -7.9804e-02,  7.2587e-02,\n",
       "                        -2.0253e-02,  2.1258e-02,  2.0052e-01,  2.7835e-01, -3.8611e-02,\n",
       "                         1.7087e-01,  1.4004e-01, -4.2077e-02, -1.2705e-01, -7.0402e-03,\n",
       "                        -5.7980e-02,  2.8442e-02,  1.6958e-01,  1.1434e-01, -1.1726e-01,\n",
       "                         5.0130e-02, -1.6752e-01,  1.6811e-02,  1.0937e-01, -3.0701e-01,\n",
       "                         9.0780e-02,  4.1779e-01,  2.5580e-01,  1.2615e-01,  1.4613e-01,\n",
       "                        -2.2526e-01,  4.3236e-01,  1.0323e-02,  4.2330e-02, -2.7016e-01,\n",
       "                        -9.6861e-02,  8.8947e-02,  2.4410e-01,  1.9409e-01, -6.8399e-02,\n",
       "                         7.5153e-02, -9.8173e-02, -9.3322e-02,  1.7011e-01, -9.0170e-02,\n",
       "                         1.2302e-01,  1.2475e-01, -1.3442e-02,  1.2194e-01,  2.0767e-01,\n",
       "                        -2.9880e-01, -1.8974e-01, -9.4217e-02, -2.4025e-01, -7.4426e-02,\n",
       "                         4.6801e-02, -3.3186e-03, -1.1543e-01,  1.5463e-01,  1.0589e-01,\n",
       "                        -1.1272e-01, -4.3528e-02, -1.2754e-01,  6.9504e-02, -6.7879e-03,\n",
       "                        -6.7565e-03, -5.5609e-02, -3.2987e-02,  3.7985e-02,  1.3538e-01,\n",
       "                        -7.4057e-02,  1.1568e-01,  2.5219e-01, -8.2575e-02,  6.8472e-02,\n",
       "                         1.0038e-01, -7.5474e-03,  1.1348e-01,  1.8473e-01, -4.4852e-02,\n",
       "                         2.0643e-01,  4.1199e-02,  9.5827e-02, -1.9974e-01, -1.2097e-01,\n",
       "                         2.3303e-01, -1.1992e-01,  1.5486e-01,  8.2433e-02, -5.6376e-02,\n",
       "                         4.4192e-01, -2.8592e-01,  9.8825e-02,  1.1151e-01,  8.2085e-02,\n",
       "                         1.0692e-01,  2.0098e-01, -2.4095e-02, -2.1286e-02, -6.9871e-02],\n",
       "                       [-5.3393e-02, -2.1040e-01,  2.0645e-02, -1.2158e-01,  6.4295e-02,\n",
       "                         3.5534e-01,  6.8538e-02, -1.1077e-01,  8.5480e-02, -2.7764e-02,\n",
       "                        -4.8541e-02,  5.3556e-04, -8.5307e-02, -9.7926e-02,  3.6551e-01,\n",
       "                         8.2160e-02, -1.8504e-01, -1.5577e-01,  2.3410e-03,  1.0111e-01,\n",
       "                        -2.4764e-01, -1.7520e-01, -3.1858e-02, -1.9193e-01,  5.2591e-02,\n",
       "                         1.5206e-01, -1.1653e-01,  1.2989e-01,  4.4978e-01, -2.1562e-01,\n",
       "                         1.0640e-01,  8.3629e-02,  1.6812e-01, -4.4979e-02,  1.4376e-01,\n",
       "                         5.0180e-02,  1.4680e-01, -1.0265e-01,  2.7306e-02,  1.4638e-03,\n",
       "                         1.5697e-03,  1.6758e-01,  9.4077e-02,  8.6761e-02,  2.1308e-02,\n",
       "                        -7.5383e-02,  1.5285e-01, -1.4682e-01, -4.8322e-03,  2.6243e-01,\n",
       "                         2.5513e-01,  1.1535e-01,  1.2473e-01,  1.1624e-01,  1.5059e-01,\n",
       "                         7.1312e-02, -7.3678e-02, -1.9997e-01, -2.2240e-01,  1.4173e-01,\n",
       "                        -1.0390e-01, -3.9350e-02,  3.9242e-02, -1.2706e-01,  6.7331e-03,\n",
       "                        -1.3828e-01,  1.8857e-01,  2.4268e-02, -1.5787e-01,  4.3103e-02,\n",
       "                        -1.2088e-01, -8.6138e-03, -4.1796e-02,  9.8679e-02, -1.3846e-01,\n",
       "                         8.9635e-02, -7.1376e-02,  7.4177e-02,  1.6940e-02, -6.9578e-03,\n",
       "                        -9.7672e-03, -2.4641e-01,  1.4560e-01,  1.8213e-01,  3.0395e-01,\n",
       "                         1.7132e-01,  1.1679e-01, -2.0633e-02,  2.1945e-01, -1.1106e-02,\n",
       "                        -1.1913e-01, -7.7336e-02,  1.5785e-01,  4.2180e-02, -3.8443e-02,\n",
       "                         1.8650e-01, -6.6146e-02, -1.0824e-01,  1.2724e-02,  6.6072e-02],\n",
       "                       [-5.2662e-02, -1.0055e-01, -9.0194e-02,  1.3973e-03,  9.3246e-02,\n",
       "                         1.1604e-01, -1.0062e-01, -1.3041e-01,  2.3973e-02,  2.1488e-01,\n",
       "                         2.2456e-01, -5.2126e-02, -2.4382e-02, -1.6996e-02,  1.2378e-01,\n",
       "                         2.6762e-01, -1.9445e-01, -6.7271e-02, -3.9806e-02,  1.3304e-01,\n",
       "                         6.1281e-02,  8.4152e-02,  1.0012e-01,  6.5152e-02, -5.3040e-02,\n",
       "                         7.3251e-02, -5.3601e-02, -8.2222e-02, -2.6449e-01, -2.5106e-01,\n",
       "                        -1.0055e-01, -6.7879e-02,  7.7237e-02, -1.5192e-01, -2.1533e-01,\n",
       "                         1.2027e-01,  3.9710e-02,  6.3940e-02,  4.1243e-02, -1.7776e-02,\n",
       "                        -1.0272e-01,  2.1362e-01,  2.8712e-01,  1.2860e-01,  6.7421e-02,\n",
       "                         3.9320e-01, -1.6024e-01,  3.3894e-01, -2.6779e-02, -1.7210e-01,\n",
       "                        -1.2247e-01, -7.8528e-03, -3.0497e-01, -4.2846e-03,  6.2824e-02,\n",
       "                        -3.4572e-02,  1.8385e-01,  1.7341e-01, -2.1712e-02, -1.6382e-01,\n",
       "                         2.2259e-02,  1.4020e-01, -1.7245e-02, -8.1855e-02, -8.2866e-02,\n",
       "                         9.5083e-02,  9.9719e-02,  2.1899e-02,  3.1099e-01, -1.5444e-01,\n",
       "                        -1.0705e-01, -6.6520e-02, -2.4870e-01, -2.4215e-01,  1.6928e-01,\n",
       "                         1.6741e-01,  6.3780e-03, -7.5318e-02, -1.1915e-01, -3.6076e-01,\n",
       "                        -1.2479e-01, -2.8883e-02,  1.9024e-01,  5.7501e-02, -1.9126e-01,\n",
       "                         3.2964e-01, -4.4764e-02, -4.5548e-02,  2.9705e-01,  4.9624e-03,\n",
       "                        -8.0098e-02,  1.6792e-01, -1.1715e-01,  3.8010e-02,  1.5543e-01,\n",
       "                        -4.0450e-03,  7.8966e-02,  2.3815e-01, -1.1594e-01,  3.3925e-02]],\n",
       "                      device='cuda:0')),\n",
       "              ('fc2.bias',\n",
       "               tensor([-0.0024, -0.0126,  0.0123,  0.0036, -0.0045,  0.0033,  0.0102, -0.0085,\n",
       "                       -0.0035,  0.0020], device='cuda:0'))]),\n",
       " 'fedavg_hyperparameters': {'learning_rate': 0.01,\n",
       "  'local_update_steps': 5,\n",
       "  'sgd_batch_size': 50,\n",
       "  'participation_rate': 1.0},\n",
       " 'user_with_data': ''}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_record(config, record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffusionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
