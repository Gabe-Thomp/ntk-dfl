{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFedAvgM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "directory_path = \"../../../\"\n",
    "if directory_path not in sys.path:\n",
    "    # Add the directory to sys.path\n",
    "    sys.path.append(directory_path)\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils import data\n",
    "from torch import optim\n",
    "\n",
    "from utils.utils import *\n",
    "from utils import load_config\n",
    "from utils.validate import *\n",
    "from fedlearning.model import *\n",
    "from fedlearning.dataset import *\n",
    "from fedlearning.evolve import *\n",
    "from fedlearning.optimizer import GlobalUpdater, LocalUpdater, get_omegas\n",
    "from fedlearning.quantizer import SqcCompressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_average(client_id, neighbors, model_dict, verbose=False):\n",
    "        averaged_weight = average_neighbor_weights(client_id, neighbors, model_dict)\n",
    "        # Load weight into client\n",
    "        model_dict[client_id].load_state_dict(averaged_weight)\n",
    "\n",
    "\n",
    "def create_random_graph(n, p, graph_name=None):\n",
    "    # Generate the graph\n",
    "    G = nx.erdos_renyi_graph(n, p)\n",
    "\n",
    "    if graph_name != None:\n",
    "        # Draw the graph\n",
    "        nx.draw(G, with_labels=True)\n",
    "        plt.savefig(graph_name)\n",
    "    return G\n",
    "\n",
    "def create_ring_graph(n, graph_name=None):\n",
    "    G = nx.cycle_graph(n)\n",
    "    if graph_name != None:\n",
    "        # Draw the graph\n",
    "        nx.draw(G, with_labels=True)\n",
    "        plt.savefig(graph_name)\n",
    "    return G\n",
    "\n",
    "def create_regular_graph(n, d, graph_name=None):\n",
    "    # Generate the graph\n",
    "    G = nx.random_regular_graph(d, n)\n",
    "    \n",
    "    if graph_name != None:\n",
    "        # Draw the graph\n",
    "        nx.draw(G, with_labels=True)\n",
    "        plt.savefig(graph_name)\n",
    "    return G\n",
    "\n",
    "def average_neighbor_weights(client_id, neighbor_ids, model_dict):\n",
    "    # Average the weights of the models in the cluster\n",
    "    weight_dict = copy.deepcopy(model_dict[client_id].state_dict())\n",
    "    weight_aggregator = WeightMod(weight_dict)\n",
    "    for user_id in neighbor_ids:\n",
    "        weight_aggregator.add(copy.deepcopy(model_dict[user_id].state_dict()))\n",
    "    # Add one for the client itself\n",
    "    weight_aggregator.mul(1.0/ (len(neighbor_ids)+1) )\n",
    "    return weight_aggregator.state_dict()\n",
    "\n",
    "\n",
    "def load_and_deload_neighbor_weights(neighbor_ids, model_dict, avg_weight_dict):\n",
    "    # Save the weights of the neighbors\n",
    "    older_weight_dicts = [copy.deepcopy(model_dict[user_id].state_dict()) for user_id in neighbor_ids]\n",
    "    # Load the average weights\n",
    "    for user_id in neighbor_ids:\n",
    "        model_dict[user_id].load_state_dict(avg_weight_dict)\n",
    "    return older_weight_dicts\n",
    "\n",
    "def reload_neighbor_weights(neighbor_ids, model_dict, old_weight_dicts):\n",
    "    for i, user_id in enumerate(neighbor_ids):\n",
    "        model_dict[user_id].load_state_dict(old_weight_dicts[i])\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy array): Array of data samples.\n",
    "            targets (numpy array): Array of labels corresponding to the data samples.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, target\n",
    "\n",
    "def numpy_to_tensor_transform(data):\n",
    "    return torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_client_momentum(user_model, user_id, dataset, config, logger, loss_fn, \n",
    "sgd_batch_size=32, local_epochs=1, lr = 0.01, momentum = 0.99, verbose=False): \n",
    "    # Get data corresponding to a certain user\n",
    "    user_resource = assign_user_resource(config, user_id, \n",
    "                        dataset[\"train_data\"], dataset[\"user_with_data\"])\n",
    "    \n",
    "    # Define the momentum optimizer\n",
    "    # Following the DFedAvgM paper\n",
    "    optimizer = optim.SGD(user_model.parameters(), lr=lr, momentum=momentum, \n",
    "    weight_decay=0, dampening=0, nesterov=False)\n",
    "    np_dataset = NumpyDataset(user_resource[\"images\"], user_resource[\"labels\"], transform=numpy_to_tensor_transform)\n",
    "\n",
    "    user_data_loader = DataLoader(np_dataset, batch_size=sgd_batch_size, shuffle=True)\n",
    "   \n",
    "    # Doing local_epochs number of local training rounds\n",
    "    for epoch in range(local_epochs):\n",
    "        # Iterate over the user's data\n",
    "        for batch_idx, (data, target) in enumerate(user_data_loader):\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = user_model(data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = loss_fn(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                if verbose: logger.info(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(user_data_loader.dataset)} ({100. * batch_idx / len(user_data_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    if verbose: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Loaded configuration from baseline_configs/config_dfedavgm.yaml\n",
      "Dataset path: ../../../data/mnist/train.dat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating config from filepath:  baseline_configs/config_dfedavgm.yaml\n",
      "/home/gathomp3/Deep_Learning/NeuralTangent/ntk-fed/notebooks/baselines/dfedavgm/../../../../records/baseline_trials/dfedavgm/trial_test/train.log\n"
     ]
    }
   ],
   "source": [
    "config_file = \"baseline_configs/config_dfedavgm.yaml\"\n",
    "config = load_config(config_file)\n",
    "\n",
    "logger = init_logger(config)\n",
    "logger.info(\"Loaded configuration from {}\".format(config_file))\n",
    "logger.info(\"Dataset path: {}\".format(config.train_data_dir))\n",
    "\n",
    "# Define a model to extract number of parameters for record\n",
    "if config.record_path is not None:\n",
    "    record = load_record(config.record_path)\n",
    "    logger.info(\"Loaded record from {}\".format(config.record_path))\n",
    "    loaded_record = True\n",
    "else:\n",
    "    model = init_model(config, logger)\n",
    "    record = init_record(config, model)\n",
    "    loaded_record = False\n",
    "\n",
    "if config.device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create user_ids and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Non-IID data distribution\n",
      "Load user_with_data from /home/gathomp3/Deep_Learning/NeuralTangent/ntk-fed/data/user_with_data/mnist300/a0.5/user_dataidx_map_0.50_0.dat\n"
     ]
    }
   ],
   "source": [
    "# Create user_ids\n",
    "user_ids = np.arange(0, config.users)\n",
    "# load the dataset\n",
    "# dataset object is a dictionary with keys: train_data, test_data, user_with_data\n",
    "# user_with_data is a dictionary with keys: userID:sampleID\n",
    "# For example, in the IID setting ID's are just assigned like 0, 1, 2, 3, ...\n",
    "dataset = assign_user_data(config, logger)\n",
    "test_images = torch.from_numpy(dataset[\"test_data\"][\"images\"]).to(config.device)\n",
    "test_labels = torch.from_numpy(dataset[\"test_data\"][\"labels\"]).to(config.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize user model dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_record = False\n",
    "# Create a dictionary of models for each user\n",
    "# Same initialization for all users\n",
    "# If record/model_dict is passed, continue training from where it left off\n",
    "if loaded_record == True:\n",
    "    model_dict = record[\"models\"]\n",
    "else:\n",
    "    if config.same_init:\n",
    "        model = init_model(config, logger)\n",
    "        model_dict = {model_id: copy.deepcopy(model) for model_id in user_ids}\n",
    "    else:\n",
    "        model_dict = {model_id: init_model(config, logger) for model_id in user_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get zeroth round loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging initial loss, acc\n",
      "client 0 loss 2.8094 acc 0.0798\n",
      "client 1 loss 2.9916 acc 0.0798\n",
      "client 2 loss 3.0223 acc 0.0798\n",
      "client 3 loss 2.9153 acc 0.0798\n",
      "client 4 loss 2.8716 acc 0.0798\n",
      "client 5 loss 2.8487 acc 0.0798\n",
      "client 6 loss 2.8266 acc 0.0798\n",
      "client 7 loss 2.5721 acc 0.0798\n",
      "client 8 loss 3.3079 acc 0.0798\n",
      "client 9 loss 2.6787 acc 0.0798\n",
      "client 10 loss 3.1393 acc 0.0798\n",
      "client 11 loss 3.1946 acc 0.0798\n",
      "client 12 loss 2.8435 acc 0.0798\n",
      "client 13 loss 2.6586 acc 0.0798\n",
      "client 14 loss 2.3556 acc 0.0798\n",
      "client 15 loss 2.6228 acc 0.0798\n",
      "client 16 loss 2.9121 acc 0.0798\n",
      "client 17 loss 2.6320 acc 0.0798\n",
      "client 18 loss 2.9415 acc 0.0798\n",
      "client 19 loss 2.9095 acc 0.0798\n",
      "client 20 loss 3.3225 acc 0.0798\n",
      "client 21 loss 2.5120 acc 0.0798\n",
      "client 22 loss 2.6807 acc 0.0798\n",
      "client 23 loss 2.8600 acc 0.0798\n",
      "client 24 loss 2.6811 acc 0.0798\n",
      "client 25 loss 3.5398 acc 0.0798\n",
      "client 26 loss 2.5985 acc 0.0798\n",
      "client 27 loss 3.4576 acc 0.0798\n",
      "client 28 loss 2.6968 acc 0.0798\n",
      "client 29 loss 3.2190 acc 0.0798\n",
      "client 30 loss 2.8774 acc 0.0798\n",
      "client 31 loss 3.0683 acc 0.0798\n",
      "client 32 loss 3.0664 acc 0.0798\n",
      "client 33 loss 3.0295 acc 0.0798\n",
      "client 34 loss 3.2312 acc 0.0798\n",
      "client 35 loss 3.2796 acc 0.0798\n",
      "client 36 loss 2.5451 acc 0.0798\n",
      "client 37 loss 2.9978 acc 0.0798\n",
      "client 38 loss 3.0772 acc 0.0798\n",
      "client 39 loss 3.2036 acc 0.0798\n",
      "client 40 loss 2.5647 acc 0.0798\n",
      "client 41 loss 2.7165 acc 0.0798\n",
      "client 42 loss 2.9178 acc 0.0798\n",
      "client 43 loss 2.6286 acc 0.0798\n",
      "client 44 loss 2.8487 acc 0.0798\n",
      "client 45 loss 2.4368 acc 0.0798\n",
      "client 46 loss 2.6912 acc 0.0798\n",
      "client 47 loss 2.6105 acc 0.0798\n",
      "client 48 loss 2.7916 acc 0.0798\n",
      "client 49 loss 2.6950 acc 0.0798\n",
      "client 50 loss 2.8665 acc 0.0798\n",
      "client 51 loss 3.0540 acc 0.0798\n",
      "client 52 loss 3.1351 acc 0.0798\n",
      "client 53 loss 3.0594 acc 0.0798\n",
      "client 54 loss 2.8518 acc 0.0798\n",
      "client 55 loss 2.7417 acc 0.0798\n",
      "client 56 loss 2.7688 acc 0.0798\n",
      "client 57 loss 2.4229 acc 0.0798\n",
      "client 58 loss 2.7006 acc 0.0798\n",
      "client 59 loss 3.1887 acc 0.0798\n",
      "client 60 loss 2.5291 acc 0.0798\n",
      "client 61 loss 2.4890 acc 0.0798\n",
      "client 62 loss 3.7293 acc 0.0798\n",
      "client 63 loss 2.3952 acc 0.0798\n",
      "client 64 loss 3.1148 acc 0.0798\n",
      "client 65 loss 2.6609 acc 0.0798\n",
      "client 66 loss 3.5202 acc 0.0798\n",
      "client 67 loss 2.4985 acc 0.0798\n",
      "client 68 loss 2.7077 acc 0.0798\n",
      "client 69 loss 2.8002 acc 0.0798\n",
      "client 70 loss 3.1077 acc 0.0798\n",
      "client 71 loss 2.4151 acc 0.0798\n",
      "client 72 loss 2.9890 acc 0.0798\n",
      "client 73 loss 2.5397 acc 0.0798\n",
      "client 74 loss 2.9412 acc 0.0798\n",
      "client 75 loss 2.4569 acc 0.0798\n",
      "client 76 loss 3.1977 acc 0.0798\n",
      "client 77 loss 2.7286 acc 0.0798\n",
      "client 78 loss 3.2068 acc 0.0798\n",
      "client 79 loss 2.6074 acc 0.0798\n",
      "client 80 loss 2.8665 acc 0.0798\n",
      "client 81 loss 2.3407 acc 0.0798\n",
      "client 82 loss 2.6495 acc 0.0798\n",
      "client 83 loss 3.1444 acc 0.0798\n",
      "client 84 loss 2.5973 acc 0.0798\n",
      "client 85 loss 3.1563 acc 0.0798\n",
      "client 86 loss 2.8086 acc 0.0798\n",
      "client 87 loss 3.5391 acc 0.0798\n",
      "client 88 loss 2.6625 acc 0.0798\n",
      "client 89 loss 2.6060 acc 0.0798\n",
      "client 90 loss 3.2025 acc 0.0798\n",
      "client 91 loss 3.0031 acc 0.0798\n",
      "client 92 loss 3.3169 acc 0.0798\n",
      "client 93 loss 3.5367 acc 0.0798\n",
      "client 94 loss 2.5807 acc 0.0798\n",
      "client 95 loss 2.8821 acc 0.0798\n",
      "client 96 loss 2.6517 acc 0.0798\n",
      "client 97 loss 3.7346 acc 0.0798\n",
      "client 98 loss 3.0347 acc 0.0798\n",
      "client 99 loss 3.4654 acc 0.0798\n",
      "client 100 loss 3.5738 acc 0.0798\n",
      "client 101 loss 2.6512 acc 0.0798\n",
      "client 102 loss 2.8431 acc 0.0798\n",
      "client 103 loss 2.7309 acc 0.0798\n",
      "client 104 loss 2.8990 acc 0.0798\n",
      "client 105 loss 2.8621 acc 0.0798\n",
      "client 106 loss 2.9167 acc 0.0798\n",
      "client 107 loss 2.5443 acc 0.0798\n",
      "client 108 loss 2.7176 acc 0.0798\n",
      "client 109 loss 2.7333 acc 0.0798\n",
      "client 110 loss 3.5281 acc 0.0798\n",
      "client 111 loss 3.3467 acc 0.0798\n",
      "client 112 loss 3.3746 acc 0.0798\n",
      "client 113 loss 2.7694 acc 0.0798\n",
      "client 114 loss 2.7581 acc 0.0798\n",
      "client 115 loss 2.5835 acc 0.0798\n",
      "client 116 loss 3.0275 acc 0.0798\n",
      "client 117 loss 2.7664 acc 0.0798\n",
      "client 118 loss 3.1924 acc 0.0798\n",
      "client 119 loss 3.2487 acc 0.0798\n",
      "client 120 loss 3.2083 acc 0.0798\n",
      "client 121 loss 3.4066 acc 0.0798\n",
      "client 122 loss 3.1664 acc 0.0798\n",
      "client 123 loss 2.7351 acc 0.0798\n",
      "client 124 loss 2.8898 acc 0.0798\n",
      "client 125 loss 2.7404 acc 0.0798\n",
      "client 126 loss 2.9241 acc 0.0798\n",
      "client 127 loss 2.9850 acc 0.0798\n",
      "client 128 loss 2.5419 acc 0.0798\n",
      "client 129 loss 3.0972 acc 0.0798\n",
      "client 130 loss 3.7153 acc 0.0798\n",
      "client 131 loss 2.5988 acc 0.0798\n",
      "client 132 loss 4.0021 acc 0.0798\n",
      "client 133 loss 2.6870 acc 0.0798\n",
      "client 134 loss 3.1269 acc 0.0798\n",
      "client 135 loss 2.9514 acc 0.0798\n",
      "client 136 loss 3.7272 acc 0.0798\n",
      "client 137 loss 2.6544 acc 0.0798\n",
      "client 138 loss 2.8564 acc 0.0798\n",
      "client 139 loss 3.9832 acc 0.0798\n",
      "client 140 loss 3.2367 acc 0.0798\n",
      "client 141 loss 2.7996 acc 0.0798\n",
      "client 142 loss 3.3335 acc 0.0798\n",
      "client 143 loss 2.5319 acc 0.0798\n",
      "client 144 loss 2.7046 acc 0.0798\n",
      "client 145 loss 3.2063 acc 0.0798\n",
      "client 146 loss 3.0577 acc 0.0798\n",
      "client 147 loss 2.8989 acc 0.0798\n",
      "client 148 loss 3.0556 acc 0.0798\n",
      "client 149 loss 2.4750 acc 0.0798\n",
      "client 150 loss 3.1875 acc 0.0798\n",
      "client 151 loss 2.8799 acc 0.0798\n",
      "client 152 loss 2.4714 acc 0.0798\n",
      "client 153 loss 3.3228 acc 0.0798\n",
      "client 154 loss 2.5752 acc 0.0798\n",
      "client 155 loss 3.1784 acc 0.0798\n",
      "client 156 loss 3.1482 acc 0.0798\n",
      "client 157 loss 2.7324 acc 0.0798\n",
      "client 158 loss 2.6864 acc 0.0798\n",
      "client 159 loss 3.2724 acc 0.0798\n",
      "client 160 loss 2.8418 acc 0.0798\n",
      "client 161 loss 3.6948 acc 0.0798\n",
      "client 162 loss 2.4792 acc 0.0798\n",
      "client 163 loss 2.9258 acc 0.0798\n",
      "client 164 loss 3.0087 acc 0.0798\n",
      "client 165 loss 2.7505 acc 0.0798\n",
      "client 166 loss 2.6517 acc 0.0798\n",
      "client 167 loss 2.6081 acc 0.0798\n",
      "client 168 loss 2.8477 acc 0.0798\n",
      "client 169 loss 3.3240 acc 0.0798\n",
      "client 170 loss 2.8665 acc 0.0798\n",
      "client 171 loss 3.0549 acc 0.0798\n",
      "client 172 loss 2.8854 acc 0.0798\n",
      "client 173 loss 3.1075 acc 0.0798\n",
      "client 174 loss 2.5570 acc 0.0798\n",
      "client 175 loss 2.7091 acc 0.0798\n",
      "client 176 loss 2.9858 acc 0.0798\n",
      "client 177 loss 2.4539 acc 0.0798\n",
      "client 178 loss 2.9512 acc 0.0798\n",
      "client 179 loss 3.2040 acc 0.0798\n",
      "client 180 loss 3.2654 acc 0.0798\n",
      "client 181 loss 2.5458 acc 0.0798\n",
      "client 182 loss 3.6226 acc 0.0798\n",
      "client 183 loss 3.1038 acc 0.0798\n",
      "client 184 loss 3.6031 acc 0.0798\n",
      "client 185 loss 3.1898 acc 0.0798\n",
      "client 186 loss 2.6768 acc 0.0798\n",
      "client 187 loss 2.9165 acc 0.0798\n",
      "client 188 loss 3.8171 acc 0.0798\n",
      "client 189 loss 3.1857 acc 0.0798\n",
      "client 190 loss 2.6468 acc 0.0798\n",
      "client 191 loss 3.5716 acc 0.0798\n",
      "client 192 loss 2.8781 acc 0.0798\n",
      "client 193 loss 2.6642 acc 0.0798\n",
      "client 194 loss 3.3493 acc 0.0798\n",
      "client 195 loss 2.8423 acc 0.0798\n",
      "client 196 loss 3.4778 acc 0.0798\n",
      "client 197 loss 2.8060 acc 0.0798\n",
      "client 198 loss 3.7095 acc 0.0798\n",
      "client 199 loss 3.6802 acc 0.0798\n",
      "client 200 loss 2.6480 acc 0.0798\n",
      "client 201 loss 2.6695 acc 0.0798\n",
      "client 202 loss 3.1263 acc 0.0798\n",
      "client 203 loss 2.4957 acc 0.0798\n",
      "client 204 loss 2.4418 acc 0.0798\n",
      "client 205 loss 2.8557 acc 0.0798\n",
      "client 206 loss 2.7784 acc 0.0798\n",
      "client 207 loss 2.8986 acc 0.0798\n",
      "client 208 loss 2.9863 acc 0.0798\n",
      "client 209 loss 2.8378 acc 0.0798\n",
      "client 210 loss 2.8974 acc 0.0798\n",
      "client 211 loss 2.7791 acc 0.0798\n",
      "client 212 loss 2.6364 acc 0.0798\n",
      "client 213 loss 2.6066 acc 0.0798\n",
      "client 214 loss 2.9866 acc 0.0798\n",
      "client 215 loss 2.6044 acc 0.0798\n",
      "client 216 loss 2.8540 acc 0.0798\n",
      "client 217 loss 3.2475 acc 0.0798\n",
      "client 218 loss 2.7076 acc 0.0798\n",
      "client 219 loss 3.4658 acc 0.0798\n",
      "client 220 loss 2.7201 acc 0.0798\n",
      "client 221 loss 2.7964 acc 0.0798\n",
      "client 222 loss 2.6524 acc 0.0798\n",
      "client 223 loss 3.3323 acc 0.0798\n",
      "client 224 loss 3.0108 acc 0.0798\n",
      "client 225 loss 3.3584 acc 0.0798\n",
      "client 226 loss 2.5372 acc 0.0798\n",
      "client 227 loss 2.5621 acc 0.0798\n",
      "client 228 loss 3.0040 acc 0.0798\n",
      "client 229 loss 2.5256 acc 0.0798\n",
      "client 230 loss 3.9387 acc 0.0798\n",
      "client 231 loss 3.4952 acc 0.0798\n",
      "client 232 loss 2.8700 acc 0.0798\n",
      "client 233 loss 2.9400 acc 0.0798\n",
      "client 234 loss 2.7065 acc 0.0798\n",
      "client 235 loss 3.5798 acc 0.0798\n",
      "client 236 loss 3.2201 acc 0.0798\n",
      "client 237 loss 2.9575 acc 0.0798\n",
      "client 238 loss 2.5675 acc 0.0798\n",
      "client 239 loss 2.8069 acc 0.0798\n",
      "client 240 loss 3.1711 acc 0.0798\n",
      "client 241 loss 2.8846 acc 0.0798\n",
      "client 242 loss 2.3752 acc 0.0798\n",
      "client 243 loss 2.5684 acc 0.0798\n",
      "client 244 loss 2.6578 acc 0.0798\n",
      "client 245 loss 3.9020 acc 0.0798\n",
      "client 246 loss 3.1874 acc 0.0798\n",
      "client 247 loss 2.6114 acc 0.0798\n",
      "client 248 loss 3.6465 acc 0.0798\n",
      "client 249 loss 2.6745 acc 0.0798\n",
      "client 250 loss 3.4624 acc 0.0798\n",
      "client 251 loss 2.6582 acc 0.0798\n",
      "client 252 loss 3.2778 acc 0.0798\n",
      "client 253 loss 2.4321 acc 0.0798\n",
      "client 254 loss 2.4461 acc 0.0798\n",
      "client 255 loss 3.2758 acc 0.0798\n",
      "client 256 loss 2.6746 acc 0.0798\n",
      "client 257 loss 2.8719 acc 0.0798\n",
      "client 258 loss 3.2194 acc 0.0798\n",
      "client 259 loss 2.8821 acc 0.0798\n",
      "client 260 loss 2.5154 acc 0.0798\n",
      "client 261 loss 2.8718 acc 0.0798\n",
      "client 262 loss 3.1925 acc 0.0798\n",
      "client 263 loss 2.7482 acc 0.0798\n",
      "client 264 loss 3.5338 acc 0.0798\n",
      "client 265 loss 2.6925 acc 0.0798\n",
      "client 266 loss 2.8852 acc 0.0798\n",
      "client 267 loss 2.9834 acc 0.0798\n",
      "client 268 loss 2.6551 acc 0.0798\n",
      "client 269 loss 2.7780 acc 0.0798\n",
      "client 270 loss 2.8367 acc 0.0798\n",
      "client 271 loss 2.5954 acc 0.0798\n",
      "client 272 loss 2.6674 acc 0.0798\n",
      "client 273 loss 2.6498 acc 0.0798\n",
      "client 274 loss 2.6107 acc 0.0798\n",
      "client 275 loss 3.2815 acc 0.0798\n",
      "client 276 loss 2.4719 acc 0.0798\n",
      "client 277 loss 2.3701 acc 0.0798\n",
      "client 278 loss 2.7650 acc 0.0798\n",
      "client 279 loss 3.0447 acc 0.0798\n",
      "client 280 loss 2.5193 acc 0.0798\n",
      "client 281 loss 3.0888 acc 0.0798\n",
      "client 282 loss 3.0534 acc 0.0798\n",
      "client 283 loss 2.8742 acc 0.0798\n",
      "client 284 loss 2.6961 acc 0.0798\n",
      "client 285 loss 2.6302 acc 0.0798\n",
      "client 286 loss 3.2014 acc 0.0798\n",
      "client 287 loss 2.8733 acc 0.0798\n",
      "client 288 loss 3.1150 acc 0.0798\n",
      "client 289 loss 3.4468 acc 0.0798\n",
      "client 290 loss 2.4678 acc 0.0798\n",
      "client 291 loss 2.8653 acc 0.0798\n",
      "client 292 loss 2.8014 acc 0.0798\n",
      "client 293 loss 3.0791 acc 0.0798\n",
      "client 294 loss 2.9817 acc 0.0798\n",
      "client 295 loss 2.9658 acc 0.0798\n",
      "client 296 loss 4.0165 acc 0.0798\n",
      "client 297 loss 3.7647 acc 0.0798\n",
      "client 298 loss 3.2872 acc 0.0798\n",
      "client 299 loss 3.0087 acc 0.0798\n"
     ]
    }
   ],
   "source": [
    "# Get zeroth round loss, acc\n",
    "verbose = True\n",
    "if record[\"epoch\"] == 0:\n",
    "    logger.info(\"Logging initial loss, acc\")\n",
    "    client_losses = []\n",
    "    client_accs = []\n",
    "    for client_id in user_ids:\n",
    "        # Evaluate the client's model on the slice of the training data corresponding to the client's data\n",
    "        user_images = torch.from_numpy(dataset[\"train_data\"][\"images\"][dataset[\"user_with_data\"][client_id]]).to(config.device)\n",
    "        user_labels = torch.from_numpy(dataset[\"train_data\"][\"labels\"][dataset[\"user_with_data\"][client_id]]).to(config.device)\n",
    "        \n",
    "        # Get model outputs\n",
    "        output_on_own_data = model_dict[client_id](user_images)\n",
    "        output_on_test_set = model_dict[client_id](test_images)\n",
    "        \n",
    "        # Get losses/accs, and append to list\n",
    "        loss = loss_with_output(output_on_own_data, user_labels, config.loss)\n",
    "        acc = accuracy_with_output(output_on_test_set, test_labels)\n",
    "        \n",
    "        client_losses.append(loss)\n",
    "        client_accs.append(acc)\n",
    "        if verbose: logger.info(\"client {:d} loss {:.4f} acc {:.4f}\".format(client_id, loss, acc))\n",
    "    # Get rid of unnecessary variables to free up memory\n",
    "    del user_images; del user_labels; del output_on_own_data; del output_on_test_set\n",
    "    # Finally, append the initial losses, accs to the record\n",
    "    record[\"loss\"].append(client_losses)\n",
    "    record[\"testing_accuracy\"].append(client_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tensor_bits(state_dict):\n",
    "    total_bits = 0\n",
    "    for name, param in state_dict.items():\n",
    "        if isinstance(param, torch.Tensor):\n",
    "            # Calculate number of elements\n",
    "            num_elements = param.numel()\n",
    "            \n",
    "            # Get number of bits per element based on data type\n",
    "            if param.dtype == torch.float32:\n",
    "                bits_per_element = 32\n",
    "            elif param.dtype == torch.float16:\n",
    "                bits_per_element = 16\n",
    "            elif param.dtype == torch.int32:\n",
    "                bits_per_element = 32\n",
    "            elif param.dtype == torch.int64:\n",
    "                bits_per_element = 64\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported data type: {param.dtype}\")\n",
    "            \n",
    "            # Calculate total bits for this tensor\n",
    "            tensor_bits = num_elements * bits_per_element\n",
    "            \n",
    "            total_bits += tensor_bits\n",
    "            \n",
    "            print(f\"{name}: {tensor_bits} bits\")\n",
    "    \n",
    "    print(f\"Total bits: {total_bits}\")\n",
    "    print(f\"Total bytes: {total_bits / 8}\")\n",
    "    print(f\"Total kilobytes: {total_bits / (8 * 1024):.2f}\")\n",
    "    print(f\"Total megabytes: {total_bits / (8 * 1024 * 1024):.2f}\")\n",
    "\n",
    "    return total_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight: 2508800 bits\n",
      "fc1.bias: 3200 bits\n",
      "fc2.weight: 32000 bits\n",
      "fc2.bias: 320 bits\n",
      "Total bits: 2544320\n",
      "Total bytes: 318040.0\n",
      "Total kilobytes: 310.59\n",
      "Total megabytes: 0.30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2544320"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_tensor_bits(model_dict[0].state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.0000, -0.0726, -0.0779,  ...,  0.0000, -0.0000, -0.0000],\n",
       "                      [-0.0000, -0.0000, -0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                      [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "                      ...,\n",
       "                      [ 0.0000,  0.0604,  0.0779,  ...,  0.0534,  0.0000, -0.0674],\n",
       "                      [-0.0000, -0.0000,  0.0000,  ...,  0.0726, -0.0000, -0.0000],\n",
       "                      [-0.0000, -0.0709,  0.0000,  ...,  0.0726, -0.0000, -0.0000]],\n",
       "                     device='cuda:0')),\n",
       "             ('fc1.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0.], device='cuda:0')),\n",
       "             ('fc2.weight',\n",
       "              tensor([[-0.3654, -0.1739, -0.2130, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       -0.0000, -0.1700,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.2989,\n",
       "                        0.1934,  0.0000, -0.2012,  0.0000,  0.2911, -0.0000,  0.1700, -0.0000,\n",
       "                        0.1934,  0.2091, -0.0000,  0.0000,  0.1739, -0.0000, -0.2051,  0.0000,\n",
       "                       -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.2442,\n",
       "                       -0.0000,  0.1661,  0.0000,  0.0000, -0.2325, -0.0000, -0.0000,  0.3615,\n",
       "                        0.0000,  0.2677,  0.0000, -0.2012, -0.0000,  0.0000,  0.0000,  0.1504,\n",
       "                       -0.1895,  0.2638, -0.0000,  0.0000, -0.0000, -0.0000, -0.2091,  0.0000,\n",
       "                       -0.4279,  0.0000,  0.0000, -0.0000, -0.3732, -0.0000,  0.0000, -0.0000,\n",
       "                       -0.0000, -0.0000,  0.1504, -0.1583, -0.1739, -0.0000, -0.0000,  0.0000,\n",
       "                        0.2442,  0.0000,  0.1465,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
       "                       -0.0000, -0.1856, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "                       -0.0000, -0.0000,  0.0000, -0.0000],\n",
       "                      [ 0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.1465, -0.0000, -0.2403,\n",
       "                       -0.2481, -0.1934,  0.0000, -0.0000, -0.0000,  0.0000, -0.2169,  0.0000,\n",
       "                        0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "                        0.1426,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.2872,  0.0000,\n",
       "                        0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.1700,\n",
       "                       -0.0000, -0.0000, -0.1973, -0.0000, -0.0000, -0.1934, -0.0000, -0.2169,\n",
       "                        0.0000, -0.1934,  0.0000, -0.0000,  0.2364,  0.0000,  0.0000,  0.0000,\n",
       "                        0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.2091,  0.2247,  0.0000,\n",
       "                        0.0000, -0.1778,  0.2012, -0.2169,  0.0000,  0.1426,  0.0000, -0.0000,\n",
       "                       -0.0000,  0.0000,  0.0000,  0.2130,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       -0.0000, -0.0000,  0.1700, -0.0000, -0.0000, -0.2091, -0.0000, -0.1817,\n",
       "                       -0.1895,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.3810,\n",
       "                       -0.0000, -0.0000, -0.0000, -0.0000],\n",
       "                      [ 0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "                       -0.0000, -0.0000,  0.0000, -0.0000, -0.1543, -0.0000, -0.0000, -0.0000,\n",
       "                       -0.1661, -0.0000, -0.2559,  0.2130,  0.2208,  0.0000, -0.0000, -0.1973,\n",
       "                       -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.1426, -0.0000,  0.1856,\n",
       "                        0.0000,  0.0000, -0.0000, -0.1700,  0.0000, -0.1817, -0.0000,  0.0000,\n",
       "                       -0.1543,  0.1504, -0.0000, -0.0000, -0.2208,  0.0000,  0.0000, -0.0000,\n",
       "                       -0.1426,  0.0000, -0.2208, -0.0000, -0.0000,  0.2755, -0.0000, -0.1622,\n",
       "                        0.0000,  0.0000, -0.0000,  0.1817,  0.0000,  0.1739,  0.3771,  0.0000,\n",
       "                        0.1426, -0.1504,  0.3224, -0.1543,  0.0000, -0.0000, -0.0000, -0.0000,\n",
       "                       -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.1583,\n",
       "                        0.0000,  0.0000, -0.0000, -0.1700,  0.0000, -0.0000,  0.2716, -0.0000,\n",
       "                       -0.2325,  0.0000,  0.0000, -0.0000,  0.2989, -0.1895,  0.0000, -0.1661,\n",
       "                       -0.0000,  0.0000, -0.0000, -0.0000],\n",
       "                      [-0.1465, -0.1934, -0.0000, -0.1973, -0.3028, -0.0000, -0.0000, -0.0000,\n",
       "                        0.2130, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.1543,\n",
       "                       -0.0000,  0.1661, -0.1895,  0.3654,  0.1934, -0.2208, -0.0000, -0.0000,\n",
       "                       -0.0000, -0.3380,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                        0.2169,  0.0000,  0.0000, -0.0000, -0.0000,  0.1583, -0.0000, -0.0000,\n",
       "                       -0.1504, -0.1973,  0.3927,  0.0000, -0.1504, -0.0000, -0.3107, -0.0000,\n",
       "                       -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.2169, -0.0000, -0.0000,\n",
       "                       -0.0000,  0.0000,  0.0000,  0.1465,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "                        0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.2638,\n",
       "                        0.1700, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "                        0.3067, -0.0000, -0.0000, -0.0000,  0.0000, -0.1817, -0.0000,  0.2012,\n",
       "                        0.0000,  0.0000,  0.1778,  0.0000, -0.1426, -0.2091,  0.1661,  0.1895,\n",
       "                        0.1622,  0.0000,  0.0000, -0.0000],\n",
       "                      [ 0.2755,  0.0000,  0.2481,  0.0000, -0.0000, -0.1817,  0.1700,  0.3185,\n",
       "                       -0.0000,  0.0000,  0.1661, -0.0000,  0.0000,  0.0000,  0.0000, -0.2755,\n",
       "                        0.0000, -0.2012, -0.0000,  0.2325,  0.1895,  0.0000,  0.1895, -0.0000,\n",
       "                       -0.0000, -0.0000, -0.0000, -0.2364,  0.3146, -0.0000,  0.3185, -0.2208,\n",
       "                       -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
       "                        0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
       "                       -0.1778,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.1622,\n",
       "                        0.1543,  0.0000,  0.1661,  0.0000,  0.0000, -0.0000,  0.1661, -0.0000,\n",
       "                       -0.0000, -0.0000, -0.0000,  0.0000, -0.3067,  0.0000,  0.0000, -0.0000,\n",
       "                        0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.1465,\n",
       "                        0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.2012, -0.1700, -0.0000,\n",
       "                       -0.0000, -0.0000, -0.0000,  0.2442, -0.1934, -0.0000,  0.0000, -0.0000,\n",
       "                       -0.0000, -0.0000, -0.0000,  0.0000],\n",
       "                      [ 0.3263, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
       "                       -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
       "                        0.0000,  0.1465, -0.0000,  0.0000,  0.1504,  0.2208, -0.0000,  0.0000,\n",
       "                        0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.2012,\n",
       "                        0.2950,  0.0000, -0.0000, -0.0000,  0.0000,  0.1465, -0.0000,  0.0000,\n",
       "                       -0.0000, -0.0000,  0.1778, -0.0000,  0.0000, -0.0000,  0.1778, -0.0000,\n",
       "                        0.0000, -0.3615,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.2911,\n",
       "                       -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
       "                        0.0000, -0.0000, -0.0000, -0.2911, -0.0000,  0.1661, -0.2286, -0.0000,\n",
       "                        0.0000,  0.1543, -0.0000,  0.0000, -0.2794, -0.0000, -0.2130, -0.1543,\n",
       "                        0.1973,  0.0000, -0.2208,  0.1465, -0.3341,  0.0000,  0.0000,  0.0000,\n",
       "                       -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.1426,\n",
       "                        0.0000, -0.0000,  0.1661,  0.0000],\n",
       "                      [-0.0000,  0.0000, -0.0000,  0.1739,  0.1543, -0.1465,  0.0000,  0.0000,\n",
       "                       -0.0000, -0.0000,  0.0000,  0.1661, -0.0000, -0.0000, -0.0000,  0.0000,\n",
       "                       -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.1700,  0.0000,  0.0000,\n",
       "                        0.3028,  0.3419,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "                       -0.0000, -0.1543, -0.1739,  0.0000, -0.0000,  0.0000, -0.0000,  0.1856,\n",
       "                        0.0000,  0.1895, -0.0000, -0.0000,  0.2091,  0.2286,  0.0000, -0.2208,\n",
       "                       -0.2247,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.1817,\n",
       "                        0.2481, -0.0000, -0.2481, -0.1856,  0.0000,  0.0000, -0.0000,  0.0000,\n",
       "                       -0.0000, -0.2403,  0.0000,  0.0000,  0.0000,  0.0000, -0.2833, -0.0000,\n",
       "                        0.0000, -0.0000, -0.0000,  0.1700,  0.0000,  0.2325,  0.3732,  0.0000,\n",
       "                        0.0000, -0.0000, -0.3302,  0.0000, -0.1895, -0.0000, -0.1778,  0.0000,\n",
       "                       -0.0000,  0.2169, -0.0000, -0.0000, -0.0000,  0.1504,  0.0000,  0.0000,\n",
       "                       -0.1622,  0.0000,  0.0000, -0.0000],\n",
       "                      [ 0.3185, -0.1856, -0.2520,  0.0000, -0.0000,  0.0000,  0.2051,  0.1465,\n",
       "                        0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.2012,  0.0000,\n",
       "                       -0.3067, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
       "                       -0.0000,  0.2833, -0.0000, -0.0000, -0.0000, -0.3146, -0.0000,  0.0000,\n",
       "                       -0.0000, -0.2051,  0.1661,  0.0000, -0.1426, -0.0000,  0.0000,  0.2169,\n",
       "                        0.0000,  0.3810, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.1973,\n",
       "                        0.1739, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.1622,\n",
       "                       -0.2012,  0.0000,  0.0000,  0.0000, -0.2599, -0.3380, -0.0000, -0.0000,\n",
       "                       -0.1583, -0.2130,  0.2520,  0.0000,  0.1856,  0.2286, -0.0000, -0.2169,\n",
       "                       -0.4982,  0.0000, -0.0000, -0.0000,  0.0000, -0.1583,  0.0000,  0.1583,\n",
       "                       -0.0000, -0.2091,  0.1895, -0.0000, -0.0000,  0.0000, -0.0000,  0.2325,\n",
       "                        0.0000, -0.2755,  0.0000,  0.2012,  0.2051,  0.1543,  0.0000,  0.0000,\n",
       "                        0.0000, -0.0000, -0.0000,  0.0000],\n",
       "                      [ 0.1778, -0.1622, -0.3146, -0.1504, -0.1504, -0.0000, -0.0000,  0.0000,\n",
       "                       -0.0000,  0.0000, -0.1583, -0.0000, -0.2950, -0.2012,  0.0000,  0.0000,\n",
       "                       -0.0000, -0.2481,  0.1973,  0.0000, -0.1895, -0.0000, -0.0000, -0.3224,\n",
       "                        0.0000,  0.3185, -0.4083, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "                        0.2012,  0.1739,  0.0000, -0.0000, -0.1700, -0.0000,  0.0000, -0.0000,\n",
       "                       -0.2051, -0.1504, -0.1856,  0.0000,  0.3146,  0.0000,  0.0000, -0.0000,\n",
       "                        0.3263, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
       "                       -0.0000,  0.1543, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
       "                        0.1583, -0.0000,  0.1543, -0.1817,  0.0000, -0.0000, -0.0000, -0.1465,\n",
       "                       -0.0000, -0.0000, -0.1739, -0.0000,  0.0000,  0.2872,  0.0000, -0.0000,\n",
       "                       -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.1934,  0.0000,\n",
       "                        0.0000,  0.0000,  0.0000,  0.0000],\n",
       "                      [-0.0000, -0.3107, -0.0000,  0.2677, -0.1934,  0.1934,  0.1973,  0.1465,\n",
       "                       -0.0000, -0.2520, -0.1778, -0.0000,  0.0000, -0.0000,  0.1622, -0.0000,\n",
       "                       -0.1739,  0.1973,  0.0000,  0.2716,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                        0.1543,  0.0000, -0.0000, -0.2794,  0.0000,  0.1622, -0.1934,  0.2403,\n",
       "                        0.1856, -0.0000, -0.0000, -0.0000,  0.2130,  0.2559,  0.1778,  0.0000,\n",
       "                        0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "                        0.0000, -0.0000,  0.0000,  0.2833, -0.0000, -0.0000,  0.0000,  0.0000,\n",
       "                        0.2716,  0.0000, -0.0000, -0.0000,  0.2833,  0.0000, -0.0000, -0.0000,\n",
       "                       -0.0000,  0.1700,  0.0000,  0.2208, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                        0.2130, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
       "                        0.2012,  0.1661, -0.1543, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
       "                       -0.0000,  0.0000, -0.1543,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
       "                       -0.1543,  0.3966, -0.1700,  0.0000]], device='cuda:0')),\n",
       "             ('fc2.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0'))])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "temp = WeightMod(model_dict[0].state_dict())\n",
    "# Define parameters\n",
    "params = {\n",
    "    \"sparsity\": 0.3,  # This means 10% of elements will be kept, 90% zeroed out\n",
    "    \"quant_level\": 256  # Number of quantization levels\n",
    "}\n",
    "\n",
    "quantizer = SqcCompressor(params=params)\n",
    "temp.apply_quant(quantizer=quantizer)\n",
    "temp.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Comm Round: 0\n",
      "/home/gathomp3/anaconda3/envs/DiffusionEnv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Round 0: Test Loss: 1.9127050638198853, Avg Client Acc: 0.28770899295806884, Agg Acc: 0.4334999918937683\n",
      "Comm Round: 1\n",
      "Comm Round: 2\n",
      "Comm Round: 3\n",
      "Comm Round: 4\n",
      "Comm Round: 5\n",
      "Round 5: Test Loss: 1.6543687582015991, Avg Client Acc: 0.4359306553006172, Agg Acc: 0.44290000200271606\n",
      "Comm Round: 6\n",
      "Comm Round: 7\n",
      "Comm Round: 8\n",
      "Comm Round: 9\n",
      "Comm Round: 10\n",
      "Round 10: Test Loss: 1.1399308443069458, Avg Client Acc: 0.5234139875570933, Agg Acc: 0.5979999899864197\n",
      "Comm Round: 11\n",
      "Comm Round: 12\n",
      "Comm Round: 13\n",
      "Comm Round: 14\n",
      "Comm Round: 15\n",
      "Round 15: Test Loss: 1.0309802293777466, Avg Client Acc: 0.559088652630647, Agg Acc: 0.6266999840736389\n",
      "Comm Round: 16\n",
      "Comm Round: 17\n",
      "Comm Round: 18\n",
      "Comm Round: 19\n",
      "Comm Round: 20\n",
      "Round 20: Test Loss: 0.9091405868530273, Avg Client Acc: 0.5946646518508594, Agg Acc: 0.6638000011444092\n",
      "Comm Round: 21\n",
      "Comm Round: 22\n",
      "Comm Round: 23\n",
      "Comm Round: 24\n",
      "Comm Round: 25\n",
      "Round 25: Test Loss: 0.8660927414894104, Avg Client Acc: 0.6194043176372847, Agg Acc: 0.6718999743461609\n",
      "Comm Round: 26\n",
      "Comm Round: 27\n",
      "Comm Round: 28\n",
      "Comm Round: 29\n",
      "Comm Round: 30\n",
      "Round 30: Test Loss: 0.7602959871292114, Avg Client Acc: 0.6395129837592443, Agg Acc: 0.7181999683380127\n",
      "Comm Round: 31\n",
      "Comm Round: 32\n",
      "Comm Round: 33\n",
      "Comm Round: 34\n",
      "Comm Round: 35\n",
      "Round 35: Test Loss: 0.7350806593894958, Avg Client Acc: 0.6467359846830368, Agg Acc: 0.7231999635696411\n",
      "Comm Round: 36\n",
      "Comm Round: 37\n",
      "Comm Round: 38\n",
      "Comm Round: 39\n",
      "Comm Round: 40\n",
      "Round 40: Test Loss: 0.7091952562332153, Avg Client Acc: 0.6496199826399486, Agg Acc: 0.7317000031471252\n",
      "Comm Round: 41\n",
      "Comm Round: 42\n",
      "Comm Round: 43\n",
      "Comm Round: 44\n",
      "Comm Round: 45\n",
      "Round 45: Test Loss: 0.7310965657234192, Avg Client Acc: 0.6557343168059985, Agg Acc: 0.7265999913215637\n",
      "Comm Round: 46\n",
      "Comm Round: 47\n",
      "Comm Round: 48\n",
      "Comm Round: 49\n"
     ]
    }
   ],
   "source": [
    "for comm_round in range(record[\"epoch\"],record[\"epoch\"]+config.rounds):\n",
    "    logger.info(f\"Comm Round: {comm_round}\")\n",
    "    client_losses = []\n",
    "    client_accs = []\n",
    "\n",
    "    # Create the graph for this round\n",
    "    if config.topology == \"random\":\n",
    "        G = create_random_graph(config.users, config.p, config.graph_name)\n",
    "    elif config.topology == \"ring\":\n",
    "        G = create_ring_graph(config.users, config.graph_name)\n",
    "    elif config.topology == \"regular\":\n",
    "        if config.p is not None:\n",
    "            raise ValueError(\"Regular graph requires d, not p\")\n",
    "        elif config.d is None:\n",
    "            raise ValueError(\"Regular graph requires d\")\n",
    "        G = create_regular_graph(config.users, config.d, config.graph_name)\n",
    "        if config.verbose: logger.info(f\"Creating regular graph with d={config.d}\")\n",
    "    else: \n",
    "        raise ValueError(\"Invalid topology: {}\".format(config.topology))\n",
    "\n",
    "    # All clients perform multiple rounds of local training\n",
    "    for client_id in user_ids:\n",
    "        # SGD w/ momentum\n",
    "        loss_fn_pytorch = nn.CrossEntropyLoss()\n",
    "        train_client_momentum(model_dict[client_id], client_id, dataset, config, logger, \n",
    "        loss_fn_pytorch, sgd_batch_size=config.sgd_batch_size, local_epochs=config.local_update_steps,\n",
    "        lr=config.lr, momentum=config.momentum, verbose=config.verbose)\n",
    "    \n",
    "    # All clients average with neighbors\n",
    "    # Must find the avged weights then load weights to mimic synchronous averaging\n",
    "    new_avged_weights = {}\n",
    "    # Get new weights for all clients\n",
    "    for client_id in user_ids:\n",
    "        neighbors = list(G.neighbors(client_id))\n",
    "        new_avged_weights[client_id] = average_neighbor_weights(client_id, neighbors, model_dict)\n",
    "    # Load new weights for all clients\n",
    "    for client_id in user_ids:\n",
    "        model_dict[client_id].load_state_dict(new_avged_weights[client_id])\n",
    "    del new_avged_weights\n",
    "    \n",
    "    # Now, test individual client accs\n",
    "    with torch.no_grad():\n",
    "        for client_id in user_ids:\n",
    "            # Get client accuracy\n",
    "            output_on_test_set = model_dict[client_id](test_images)\n",
    "            acc = accuracy_with_output(output_on_test_set, test_labels)\n",
    "            client_accs.append(acc)\n",
    "\n",
    "    # Test the global, aggregated model\n",
    "    # Note: Weighted averaging is unnecessary since all clients have the same number of samples\n",
    "    \n",
    "    # Init model to load aggregated state dict\n",
    "    temp_global_model = init_model(config, logger)\n",
    "    temp_global_model.load_state_dict(average_neighbor_weights(0, user_ids[1:], model_dict))\n",
    "    \n",
    "    global_output = temp_global_model(test_images)\n",
    "    global_loss = nn.CrossEntropyLoss()(global_output, test_labels)\n",
    "    global_acc = accuracy_with_output(global_output, test_labels)\n",
    "    \n",
    "    if comm_round % 5 == 0: \n",
    "        logger.info(f\"Round {comm_round}: Test Loss: {global_loss.item()}, Avg Client Acc: {np.mean(client_accs)}, Agg Acc: {global_acc}\")\n",
    "\n",
    "    # Record the results\n",
    "    record[\"testing_accuracy\"].append(client_accs)\n",
    "    \n",
    "    if 'aggregated_accs' in record:\n",
    "        record['aggregated_accs'].append(global_acc)\n",
    "    else:\n",
    "        record['aggregated_accs'] = [global_acc]\n",
    "\n",
    "    record[\"epoch\"] += 1\n",
    "\n",
    "# Save the record\n",
    "record[\"models\"] = model_dict\n",
    "record[\"dfedavgm_hyperparameters\"] = {\"learning_rate\": config.lr, \"local_update_steps\": config.local_update_steps,\n",
    "                                        \"sgd_batch_size\": config.sgd_batch_size, \"momentum\": config.momentum}\n",
    "record[\"user_with_data\"] = dataset[\"user_with_data\"]\n",
    "record[\"topology\"] = config.topology\n",
    "record[\"p\"] = config.p\n",
    "record[\"d\"] = config.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6800000071525574,\n",
       " 0.7443999648094177,\n",
       " 0.7813999652862549,\n",
       " 0.7925999760627747,\n",
       " 0.7997999787330627,\n",
       " 0.8075999617576599,\n",
       " 0.8172999620437622,\n",
       " 0.8193999528884888,\n",
       " 0.826200008392334,\n",
       " 0.8288999795913696,\n",
       " 0.8328999876976013,\n",
       " 0.8369999527931213,\n",
       " 0.8379999995231628,\n",
       " 0.8416000008583069,\n",
       " 0.840999960899353,\n",
       " 0.8458999991416931,\n",
       " 0.8448999524116516,\n",
       " 0.848800003528595,\n",
       " 0.8481999635696411,\n",
       " 0.8499000072479248,\n",
       " 0.8508999943733215,\n",
       " 0.8529999852180481,\n",
       " 0.8542999625205994,\n",
       " 0.8537999987602234,\n",
       " 0.8536999821662903,\n",
       " 0.8553999662399292,\n",
       " 0.8565999865531921,\n",
       " 0.8571999669075012,\n",
       " 0.8578999638557434,\n",
       " 0.8571999669075012,\n",
       " 0.8592000007629395,\n",
       " 0.8586999773979187,\n",
       " 0.8611999750137329,\n",
       " 0.8603999614715576,\n",
       " 0.8614999651908875,\n",
       " 0.8610999584197998,\n",
       " 0.8628000020980835,\n",
       " 0.8625999689102173,\n",
       " 0.8634999990463257,\n",
       " 0.8631999492645264,\n",
       " 0.8649999499320984,\n",
       " 0.8650999665260315,\n",
       " 0.8646000027656555,\n",
       " 0.866599977016449,\n",
       " 0.8662999868392944,\n",
       " 0.8661999702453613,\n",
       " 0.8669999837875366,\n",
       " 0.866599977016449,\n",
       " 0.8660999536514282,\n",
       " 0.8668999671936035]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record[\"aggregated_accs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffusionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
