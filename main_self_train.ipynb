{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "\n",
    "from torch.utils import data\n",
    "\n",
    "from utils.utils import *\n",
    "from utils import load_config\n",
    "from utils.validate import *\n",
    "from fedlearning.model import *\n",
    "from fedlearning.dataset import *\n",
    "from fedlearning.evolve import *\n",
    "from fedlearning.optimizer import GlobalUpdater, LocalUpdater, get_omegas\n",
    "from matplotlib import pyplot as plt\n",
    "from random_graph import average_neighbor_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    " # load the configuration file\n",
    "config_file = \"config.yaml\"\n",
    "config = load_config(config_file)\n",
    "\n",
    "logger = init_logger(config)\n",
    "\n",
    "model = init_model(config, logger)\n",
    "\n",
    "record = init_record(config, model)\n",
    "\n",
    "if config.device == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Non-IID data distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, loss: 3.4897, acc: 0.1166\n"
     ]
    }
   ],
   "source": [
    "user_ids = np.arange(0, config.users)\n",
    "num_participators = int(config.part_rate*config.users) \n",
    "\n",
    "# load the dataset\n",
    "# dataset object is a dictionary with keys: train_data, test_data, user_with_data\n",
    "# user_with_data is a dictionary with keys: userID:sampleID\n",
    "# For example, in the IID setting ID's are just assigned like 0, 1, 2, 3, ...\n",
    "dataset = assign_user_data(config, logger)\n",
    "test_images = torch.from_numpy(dataset[\"test_data\"][\"images\"]).to(config.device)\n",
    "test_labels = torch.from_numpy(dataset[\"test_data\"][\"labels\"]).to(config.device)\n",
    "\n",
    "# tau candidates \n",
    "taus = np.array(config.taus)\n",
    "\n",
    "# before optimization, report the result first\n",
    "\n",
    "# Get model outputs\n",
    "output_on_test_set = model(test_images)\n",
    "\n",
    "# Get losses/accs, and append to list\n",
    "loss = loss_with_output(output_on_test_set, test_labels, config.loss)\n",
    "acc = accuracy_with_output(output_on_test_set, test_labels)\n",
    "print(\"Before optimization, loss: {:.4f}, acc: {:.4f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to perform weight evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel computation time 5.368758\n",
      "loss \tacc\n",
      "1.218\t0.585\n",
      "1.161\t0.605\n",
      "1.143\t0.610\n",
      "1.133\t0.613\n",
      "1.127\t0.611\n",
      "1.124\t0.610\n",
      "1.122\t0.608\n",
      "1.121\t0.604\n",
      "1.119\t0.601\n",
      "1.118\t0.600\n",
      "1.115\t0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "current tau 1500\n",
      "acc 0.585300\n",
      "loss 1.1152\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.129\t0.560\n"
     ]
    }
   ],
   "source": [
    "# Sample random subset of users\n",
    "np.random.shuffle(user_ids)\n",
    "participator_ids = user_ids[:num_participators-1]\n",
    "\n",
    "# schedule some values to pick up\n",
    "acc = []\n",
    "losses = []\n",
    "params_list = []\n",
    "\n",
    "global_kernel = None\n",
    "global_xs = None\n",
    "global_ys = None\n",
    "local_packages = []\n",
    "local_kernels = []\n",
    "\n",
    "# Added for memory management\n",
    "global_jac = None\n",
    "for user_id in participator_ids:\n",
    "    # print(\"user {:d} updating\".format(user_id))\n",
    "\n",
    "    # assign_user_resource specifies some parameters for the user given their user_id\n",
    "    # user_resource is a dictionary with keys: lr, device, batch_size, images, labels\n",
    "    user_resource = assign_user_resource(config, user_id, \n",
    "                        dataset[\"train_data\"], dataset[\"user_with_data\"])\n",
    "    local_updater = LocalUpdater(config, user_resource)\n",
    "\n",
    "    # Gets the local jacobians for a given client specified in local_updater\n",
    "\n",
    "\n",
    "    local_updater.local_step(model)\n",
    "    # Simulate uplink transmission\n",
    "    local_package = local_updater.uplink_transmit()\n",
    "    # Append this clients jacobians to the list\n",
    "    local_packages.append(local_package)\n",
    "\n",
    "    # Send local x and y\n",
    "    if global_xs is None:\n",
    "        global_xs = local_updater.xs\n",
    "        global_ys = local_updater.ys\n",
    "    else:\n",
    "        global_xs = torch.vstack((global_xs, local_updater.xs))\n",
    "        global_ys = torch.vstack((global_ys, local_updater.ys))            \n",
    "\n",
    "    # del local_updater\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "start_time = time.time()\n",
    "global_jac = combine_local_jacobians(local_packages)\n",
    "\n",
    "del local_packages\n",
    "# Added these two lines to free up memory\n",
    "del local_package\n",
    "del local_updater\n",
    "\n",
    "global_kernel = empirical_kernel(global_jac)\n",
    "\n",
    "print(\"kernel computation time {:3f}\".format(time.time() - start_time))\n",
    "\n",
    "# Returns a function that, given t and f_0, solves for f_t\n",
    "predictor = gradient_descent_ce(global_kernel.cpu(), global_ys.cpu(), config.lr)\n",
    "\n",
    "# This is f^(0) (X)\n",
    "with torch.no_grad():\n",
    "    fx_0 = model(global_xs)\n",
    "\n",
    "# Configure maximum t as one more than the largest tau value\n",
    "t = torch.arange(config.taus[-1]+1)\n",
    "\n",
    "# Create f_x using the time values and the initial f_x\n",
    "fx_train = predictor(t, fx_0.cpu())\n",
    "# fx_train = fx_train.to(fx_0)\n",
    "\n",
    "# Use current weights to pass to the optimizer\n",
    "init_state_dict = copy.deepcopy(model.state_dict())\n",
    "\n",
    "losses = np.zeros_like(taus, dtype=float)\n",
    "acc = np.zeros_like(taus, dtype=float)\n",
    "\n",
    "print(\"loss \\tacc\")\n",
    "\n",
    "for i, tau in enumerate(config.taus):\n",
    "    # initialize the weight aggregator with current weights\n",
    "    weight_aggregator = WeightMod(init_state_dict)\n",
    "    global_omegas = get_omegas(t[:tau+1], config.lr, global_jac, \n",
    "            global_ys.cpu(), fx_train[:tau+1], config.loss, \n",
    "            model.state_dict())\n",
    "    # global_omegas = get_omegas(t[:tau+1], config.lr, global_jac, \n",
    "    #         global_ys, fx_train[:tau+1], config.loss, \n",
    "    #         model.state_dict())        \n",
    "    \n",
    "    # Complete the sum in 9b\n",
    "    weight_aggregator.add(global_omegas)\n",
    "    aggregated_weight = weight_aggregator.state_dict()\n",
    "    model.load_state_dict(aggregated_weight)\n",
    "\n",
    "    output = model(global_xs)\n",
    "\n",
    "    loss = loss_with_output(output, global_ys, config.loss)\n",
    "    # loss_fx = loss_with_output(fx_train[tau].to(global_ys), global_ys, config.loss)\n",
    "    losses[i] = loss\n",
    "\n",
    "    output = model(test_images)\n",
    "\n",
    "    test_acc = accuracy_with_output(output, test_labels)\n",
    "    acc[i] = test_acc\n",
    "\n",
    "    print(\"{:.3f}\\t{:.3f}\".format(loss, test_acc))\n",
    "\n",
    "    params_list.append(copy.deepcopy(aggregated_weight))\n",
    "\n",
    "# Get index of tau with lowest loss\n",
    "idx = np.argmin(losses)\n",
    "# Select weight parameters with lowest loss\n",
    "params = params_list[idx]\n",
    "\n",
    "# Select tau with lowest loss\n",
    "current_tau = taus[idx]\n",
    "current_acc = acc[idx]\n",
    "current_loss = losses[idx]\n",
    "\n",
    "\n",
    "logger.info(\"current tau {:d}\".format(current_tau))\n",
    "logger.info(\"acc {:4f}\".format(current_acc))\n",
    "logger.info(\"loss {:.4f}\".format(current_loss))\n",
    "# Load weights into model\n",
    "model.load_state_dict(params)\n",
    "\n",
    "# del params_list\n",
    "\n",
    "record[\"loss\"].append(current_loss)\n",
    "record[\"testing_accuracy\"].append(current_acc)\n",
    "record[\"taus\"].append(current_tau)\n",
    "\n",
    "logger.info(\"-\"*80)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After global weight evolution, loss: 1.1387, acc: 0.5853\n"
     ]
    }
   ],
   "source": [
    "# Get model outputs\n",
    "output_on_test_set = model(test_images)\n",
    "\n",
    "# Get losses/accs, and append to list\n",
    "loss = loss_with_output(output_on_test_set, test_labels, config.loss)\n",
    "acc = accuracy_with_output(output_on_test_set, test_labels)\n",
    "print(\"After global weight evolution, loss: {:.4f}, acc: {:.4f}\".format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each client performs local SGD on their own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "temp_models = {}\n",
    "for id in participator_ids:\n",
    "    temp_models[id] = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.2.1 available.\n",
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class NumpyDataset(Dataset):\n",
    "    def __init__(self, data, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy array): Array of data samples.\n",
    "            targets (numpy array): Array of labels corresponding to the data samples.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def numpy_to_tensor_transform(data):\n",
    "    return torch.from_numpy(data)\n",
    "\n",
    "\n",
    "def self_train(user_model, user_id, dataset, config, logger, loss_fn, batch_size=32, epochs=1, lr = 0.001): \n",
    "    # Get data corresponding to a certain user\n",
    "    user_resource = assign_user_resource(config, user_id, \n",
    "                        dataset[\"train_data\"], dataset[\"user_with_data\"])\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = optim.SGD(user_model.parameters(), lr=lr)\n",
    "    dataset = NumpyDataset(user_resource[\"images\"], user_resource[\"labels\"], transform=numpy_to_tensor_transform)\n",
    "\n",
    "    user_data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "   \n",
    "    for epoch in range(epochs):\n",
    "        # Iterate over the user's data\n",
    "        for batch_idx, (data, target) in enumerate(user_data_loader):\n",
    "            data, target = data.to(config.device), target.to(config.device)\n",
    "            # Clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            output = user_model(data)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = loss_fn(output, target)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(user_data_loader.dataset)} ({100. * batch_idx / len(user_data_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/196 (0%)]\tLoss: 1.742931\n",
      "Train Epoch: 1 [0/196 (0%)]\tLoss: 1.336284\n",
      "Train Epoch: 2 [0/196 (0%)]\tLoss: 1.135156\n",
      "Train Epoch: 3 [0/196 (0%)]\tLoss: 0.991975\n",
      "Train Epoch: 4 [0/196 (0%)]\tLoss: 0.935284\n",
      "\n",
      "Train Epoch: 0 [0/196 (0%)]\tLoss: 1.219621\n",
      "Train Epoch: 1 [0/196 (0%)]\tLoss: 0.845724\n",
      "Train Epoch: 2 [0/196 (0%)]\tLoss: 0.724595\n",
      "Train Epoch: 3 [0/196 (0%)]\tLoss: 0.821240\n",
      "Train Epoch: 4 [0/196 (0%)]\tLoss: 0.663198\n",
      "\n",
      "Train Epoch: 0 [0/195 (0%)]\tLoss: 1.058395\n",
      "Train Epoch: 1 [0/195 (0%)]\tLoss: 0.856083\n",
      "Train Epoch: 2 [0/195 (0%)]\tLoss: 0.629804\n",
      "Train Epoch: 3 [0/195 (0%)]\tLoss: 0.847046\n",
      "Train Epoch: 4 [0/195 (0%)]\tLoss: 0.549947\n",
      "\n",
      "Train Epoch: 0 [0/194 (0%)]\tLoss: 0.769572\n",
      "Train Epoch: 1 [0/194 (0%)]\tLoss: 0.849055\n",
      "Train Epoch: 2 [0/194 (0%)]\tLoss: 0.480561\n",
      "Train Epoch: 3 [0/194 (0%)]\tLoss: 0.744132\n",
      "Train Epoch: 4 [0/194 (0%)]\tLoss: 0.793380\n",
      "\n",
      "Train Epoch: 0 [0/196 (0%)]\tLoss: 1.217926\n",
      "Train Epoch: 1 [0/196 (0%)]\tLoss: 0.793720\n",
      "Train Epoch: 2 [0/196 (0%)]\tLoss: 0.812482\n",
      "Train Epoch: 3 [0/196 (0%)]\tLoss: 0.561344\n",
      "Train Epoch: 4 [0/196 (0%)]\tLoss: 0.517732\n",
      "\n",
      "Train Epoch: 0 [0/196 (0%)]\tLoss: 0.883058\n",
      "Train Epoch: 1 [0/196 (0%)]\tLoss: 1.042679\n",
      "Train Epoch: 2 [0/196 (0%)]\tLoss: 1.032507\n",
      "Train Epoch: 3 [0/196 (0%)]\tLoss: 0.951801\n",
      "Train Epoch: 4 [0/196 (0%)]\tLoss: 0.904654\n",
      "\n",
      "Train Epoch: 0 [0/197 (0%)]\tLoss: 1.662571\n",
      "Train Epoch: 1 [0/197 (0%)]\tLoss: 1.267708\n",
      "Train Epoch: 2 [0/197 (0%)]\tLoss: 1.178261\n",
      "Train Epoch: 3 [0/197 (0%)]\tLoss: 0.865816\n",
      "Train Epoch: 4 [0/197 (0%)]\tLoss: 1.034079\n",
      "\n",
      "Train Epoch: 0 [0/196 (0%)]\tLoss: 1.227091\n",
      "Train Epoch: 1 [0/196 (0%)]\tLoss: 0.848501\n",
      "Train Epoch: 2 [0/196 (0%)]\tLoss: 0.658576\n",
      "Train Epoch: 3 [0/196 (0%)]\tLoss: 0.868160\n",
      "Train Epoch: 4 [0/196 (0%)]\tLoss: 0.551104\n",
      "\n",
      "Train Epoch: 0 [0/196 (0%)]\tLoss: 0.555009\n",
      "Train Epoch: 1 [0/196 (0%)]\tLoss: 0.645766\n",
      "Train Epoch: 2 [0/196 (0%)]\tLoss: 0.711819\n",
      "Train Epoch: 3 [0/196 (0%)]\tLoss: 0.741455\n",
      "Train Epoch: 4 [0/196 (0%)]\tLoss: 0.585919\n",
      "\n",
      "Train Epoch: 0 [0/195 (0%)]\tLoss: 0.580664\n",
      "Train Epoch: 1 [0/195 (0%)]\tLoss: 0.638579\n",
      "Train Epoch: 2 [0/195 (0%)]\tLoss: 0.863272\n",
      "Train Epoch: 3 [0/195 (0%)]\tLoss: 0.683159\n",
      "Train Epoch: 4 [0/195 (0%)]\tLoss: 0.561598\n",
      "\n",
      "Train Epoch: 0 [0/195 (0%)]\tLoss: 1.399377\n",
      "Train Epoch: 1 [0/195 (0%)]\tLoss: 1.033566\n",
      "Train Epoch: 2 [0/195 (0%)]\tLoss: 0.978943\n",
      "Train Epoch: 3 [0/195 (0%)]\tLoss: 0.772558\n",
      "Train Epoch: 4 [0/195 (0%)]\tLoss: 0.993718\n",
      "\n",
      "Train Epoch: 0 [0/194 (0%)]\tLoss: 1.015389\n",
      "Train Epoch: 1 [0/194 (0%)]\tLoss: 0.752562\n",
      "Train Epoch: 2 [0/194 (0%)]\tLoss: 0.707789\n",
      "Train Epoch: 3 [0/194 (0%)]\tLoss: 0.632322\n",
      "Train Epoch: 4 [0/194 (0%)]\tLoss: 0.477245\n",
      "\n",
      "Train Epoch: 0 [0/194 (0%)]\tLoss: 0.293193\n",
      "Train Epoch: 1 [0/194 (0%)]\tLoss: 0.311012\n",
      "Train Epoch: 2 [0/194 (0%)]\tLoss: 0.248652\n",
      "Train Epoch: 3 [0/194 (0%)]\tLoss: 0.185281\n",
      "Train Epoch: 4 [0/194 (0%)]\tLoss: 0.171595\n",
      "\n",
      "Train Epoch: 0 [0/195 (0%)]\tLoss: 0.710384\n",
      "Train Epoch: 1 [0/195 (0%)]\tLoss: 0.525369\n",
      "Train Epoch: 2 [0/195 (0%)]\tLoss: 0.509313\n",
      "Train Epoch: 3 [0/195 (0%)]\tLoss: 0.484247\n",
      "Train Epoch: 4 [0/195 (0%)]\tLoss: 0.397869\n",
      "\n",
      "Train Epoch: 0 [0/195 (0%)]\tLoss: 1.990989\n",
      "Train Epoch: 1 [0/195 (0%)]\tLoss: 1.205065\n",
      "Train Epoch: 2 [0/195 (0%)]\tLoss: 1.170225\n",
      "Train Epoch: 3 [0/195 (0%)]\tLoss: 0.857076\n",
      "Train Epoch: 4 [0/195 (0%)]\tLoss: 0.738781\n",
      "\n",
      "Train Epoch: 0 [0/194 (0%)]\tLoss: 1.012422\n",
      "Train Epoch: 1 [0/194 (0%)]\tLoss: 0.900894\n",
      "Train Epoch: 2 [0/194 (0%)]\tLoss: 0.846131\n",
      "Train Epoch: 3 [0/194 (0%)]\tLoss: 1.053052\n",
      "Train Epoch: 4 [0/194 (0%)]\tLoss: 0.890961\n",
      "\n",
      "Train Epoch: 0 [0/196 (0%)]\tLoss: 1.196370\n",
      "Train Epoch: 1 [0/196 (0%)]\tLoss: 0.715625\n",
      "Train Epoch: 2 [0/196 (0%)]\tLoss: 1.031559\n",
      "Train Epoch: 3 [0/196 (0%)]\tLoss: 0.812333\n",
      "Train Epoch: 4 [0/196 (0%)]\tLoss: 0.696379\n",
      "\n",
      "Train Epoch: 0 [0/197 (0%)]\tLoss: 1.217388\n",
      "Train Epoch: 1 [0/197 (0%)]\tLoss: 0.846249\n",
      "Train Epoch: 2 [0/197 (0%)]\tLoss: 0.808183\n",
      "Train Epoch: 3 [0/197 (0%)]\tLoss: 0.788670\n",
      "Train Epoch: 4 [0/197 (0%)]\tLoss: 0.852452\n",
      "\n",
      "Train Epoch: 0 [0/195 (0%)]\tLoss: 1.048459\n",
      "Train Epoch: 1 [0/195 (0%)]\tLoss: 0.793428\n",
      "Train Epoch: 2 [0/195 (0%)]\tLoss: 0.787146\n",
      "Train Epoch: 3 [0/195 (0%)]\tLoss: 0.541321\n",
      "Train Epoch: 4 [0/195 (0%)]\tLoss: 0.717909\n",
      "\n",
      "Train Epoch: 0 [0/195 (0%)]\tLoss: 1.518674\n",
      "Train Epoch: 1 [0/195 (0%)]\tLoss: 1.062680\n",
      "Train Epoch: 2 [0/195 (0%)]\tLoss: 0.887016\n",
      "Train Epoch: 3 [0/195 (0%)]\tLoss: 1.006606\n",
      "Train Epoch: 4 [0/195 (0%)]\tLoss: 0.866149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn_pytorch = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "for user_id in participator_ids:\n",
    "    self_train(user_model=temp_models[user_id], user_id=user_id, dataset=dataset, config=config, logger=logger, \n",
    "               loss_fn=loss_fn_pytorch, batch_size=batch_size, epochs=epochs, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After global weight evolution, loss: 0.8727, acc: 0.6949\n",
      "After global weight evolution, loss: 1.1493, acc: 0.5974\n",
      "After global weight evolution, loss: 0.8816, acc: 0.6945\n",
      "After global weight evolution, loss: 0.9672, acc: 0.6270\n",
      "After global weight evolution, loss: 0.9917, acc: 0.6392\n",
      "After global weight evolution, loss: 0.8753, acc: 0.6883\n",
      "After global weight evolution, loss: 0.8641, acc: 0.6885\n",
      "After global weight evolution, loss: 0.9194, acc: 0.6687\n",
      "After global weight evolution, loss: 1.0822, acc: 0.6147\n",
      "After global weight evolution, loss: 0.9721, acc: 0.6453\n",
      "After global weight evolution, loss: 0.9163, acc: 0.6562\n",
      "After global weight evolution, loss: 1.0813, acc: 0.6292\n",
      "After global weight evolution, loss: 1.2199, acc: 0.5395\n",
      "After global weight evolution, loss: 1.1496, acc: 0.6144\n",
      "After global weight evolution, loss: 0.8884, acc: 0.6772\n",
      "After global weight evolution, loss: 0.8555, acc: 0.7083\n",
      "After global weight evolution, loss: 0.9000, acc: 0.6718\n",
      "After global weight evolution, loss: 0.9645, acc: 0.6558\n",
      "After global weight evolution, loss: 0.8416, acc: 0.7028\n",
      "After global weight evolution, loss: 0.8915, acc: 0.6709\n",
      "Average loss: 0.9642, Average acc: 0.6542\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "accs = []\n",
    "for user_id in participator_ids:\n",
    "    # Get model outputs\n",
    "    output_on_test_set = temp_models[user_id](test_images)\n",
    "\n",
    "    # Get losses/accs, and append to list\n",
    "    loss = loss_with_output(output_on_test_set, test_labels, config.loss)\n",
    "    acc = accuracy_with_output(output_on_test_set, test_labels)\n",
    "    print(\"After global weight evolution, loss: {:.4f}, acc: {:.4f}\".format(loss, acc))\n",
    "\n",
    "    losses.append(loss)\n",
    "    accs.append(acc)\n",
    "\n",
    "print(\"Average loss: {:.4f}, Average acc: {:.4f}\".format(np.mean(losses), np.mean(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "avged_dict = average_neighbor_weights(participator_ids[0], participator_ids[1:], temp_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After global weight evolution, loss: 0.8913, acc: 0.6740\n"
     ]
    }
   ],
   "source": [
    "avged_dict_model = init_model(config, logger)\n",
    "avged_dict_model.load_state_dict(avged_dict)\n",
    "# Get model outputs\n",
    "output_on_test_set = avged_dict_model(test_images)\n",
    "\n",
    "# Get losses/accs, and append to list\n",
    "loss = loss_with_output(output_on_test_set, test_labels, config.loss)\n",
    "acc = accuracy_with_output(output_on_test_set, test_labels)\n",
    "print(\"After global weight evolution, loss: {:.4f}, acc: {:.4f}\".format(loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DiffusionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
